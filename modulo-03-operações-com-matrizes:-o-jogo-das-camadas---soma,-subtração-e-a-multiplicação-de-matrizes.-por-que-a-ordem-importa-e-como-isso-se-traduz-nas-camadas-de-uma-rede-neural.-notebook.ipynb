{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# üéØ Opera√ß√µes com Matrizes: O Jogo das Camadas\n\n## *M√≥dulo 3: Soma, Subtra√ß√£o e Multiplica√ß√£o - Por que a Ordem Importa nas Redes Neurais*\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/algebra-linear-para-ia-modulo-03_img_01.png)\n\n---\n\n**Opa, pessoal! üëã Pedro Guth aqui!**\n\nLembra quando falamos sobre vetores no m√≥dulo anterior? T√°, mas e quando a coisa fica mais complexa? Quando temos **m√∫ltiplas dimens√µes** trabalhando juntas?\n\n√â a√≠ que entram as **opera√ß√µes com matrizes** - o verdadeiro cora√ß√£o de qualquer rede neural! \n\nPensa assim: se um vetor √© como uma **fila de pessoas**, uma matriz √© como um **pr√©dio inteiro** com v√°rias fileiras. E quando esses \"pr√©dios\" se encontram... a√≠ que a m√°gica acontece! ‚ú®"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "# Bora configurar nosso ambiente!\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Rectangle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configura√ß√£o dos gr√°ficos - deixa mais lindo!\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"üöÄ Ambiente configurado! Bora pro jogo das matrizes!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_theory"
      },
      "source": [
        "## üß† Por que Matrizes s√£o o Cora√ß√£o das Redes Neurais?\n\nT√°, mas vamos come√ßar do b√°sico. Lembra dos vetores que vimos no **M√≥dulo 2**? \n\nUma **matriz** nada mais √© que uma **cole√ß√£o organizada de vetores**! √â como se fosse um **arquivo Excel** onde cada linha ou coluna representa informa√ß√µes diferentes.\n\n### üìä A Anatomia de uma Matriz\n\nUma matriz $A$ de dimens√µes $m \\times n$ tem esta cara:\n\n$$A = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}$$\n\nOnde:\n- $m$ = n√∫mero de **linhas** (quantas \"fileiras\" temos)\n- $n$ = n√∫mero de **colunas** (quantas \"posi√ß√µes\" em cada fileira)\n- $a_{ij}$ = elemento na posi√ß√£o linha $i$, coluna $j$\n\n### üéØ Conex√£o com Redes Neurais\n\nEm uma rede neural:\n- **Cada linha** pode representar um **neur√¥nio**\n- **Cada coluna** pode representar uma **conex√£o** com a camada anterior\n- **Cada elemento** $a_{ij}$ √© o **peso** da conex√£o entre neur√¥nio $i$ e entrada $j$\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/algebra-linear-para-ia-modulo-03_img_02.png)\n\n**üî• Dica do Pedro**: Sempre pense em matrizes como **transforma√ß√µes**. Elas pegam um conjunto de dados de entrada e **transformam** em algo novo!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "basic_matrices"
      },
      "source": [
        "# Vamos criar nossas primeiras matrizes!\n",
        "print(\"üèóÔ∏è Construindo nossas matrizes exemplo\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Matriz A - Vamos imaginar que s√£o os pesos de 3 neur√¥nios conectados a 4 entradas\n",
        "A = np.array([[1, 2, 3, 4],\n",
        "              [5, 6, 7, 8],\n",
        "              [9, 10, 11, 12]])\n",
        "\n",
        "# Matriz B - Outra camada com as mesmas dimens√µes\n",
        "B = np.array([[2, 1, 4, 3],\n",
        "              [6, 5, 8, 7],\n",
        "              [10, 9, 12, 11]])\n",
        "\n",
        "print(f\"Matriz A (3√ó4):\")\n",
        "print(A)\n",
        "print(f\"\\nDimens√µes de A: {A.shape}\")\n",
        "\n",
        "print(f\"\\nMatriz B (3√ó4):\")\n",
        "print(B)\n",
        "print(f\"\\nDimens√µes de B: {B.shape}\")\n",
        "\n",
        "print(\"\\nüí° Pensa assim: cada linha √© um neur√¥nio, cada coluna uma conex√£o!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "matrix_addition"
      },
      "source": [
        "## ‚ûï Soma de Matrizes: Quando os Neur√¥nios Se Juntam\n\nA soma de matrizes √© **moleza**! √â como somar **posi√ß√£o por posi√ß√£o**, tipo quando voc√™ junta duas turmas de alunos nas mesmas carteiras.\n\n### üìê A Matem√°tica por Tr√°s\n\nPara duas matrizes $A$ e $B$ de **mesmas dimens√µes** $m \\times n$:\n\n$$C = A + B \\quad \\text{onde} \\quad c_{ij} = a_{ij} + b_{ij}$$\n\n**IMPORTANTE**: S√≥ podemos somar matrizes que t√™m **exatamente as mesmas dimens√µes**!\n\n### üß† Na Pr√°tica das Redes Neurais\n\nA soma aparece principalmente quando:\n- **Adicionamos bias** aos neur√¥nios\n- **Combinamos gradientes** durante o backpropagation\n- **Fazemos ensemble** de diferentes redes\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/algebra-linear-para-ia-modulo-03_img_03.png)\n\n**üî• Dica do Pedro**: A soma √© **comutativa** ($A + B = B + A$) e **associativa** ($A + (B + C) = (A + B) + C$). Matem√°tica linda, n√©?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "matrix_addition_code"
      },
      "source": [
        "# Bora somar nossas matrizes!\n",
        "print(\"‚ûï SOMA DE MATRIZES - O Encontro dos Neur√¥nios\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Soma elemento por elemento\n",
        "C_soma = A + B\n",
        "\n",
        "print(\"Matriz A:\")\n",
        "print(A)\n",
        "print(\"\\nMatriz B:\")\n",
        "print(B)\n",
        "print(\"\\nA + B =\")\n",
        "print(C_soma)\n",
        "\n",
        "# Vamos verificar posi√ß√£o por posi√ß√£o\n",
        "print(\"\\nüîç Verificando algumas posi√ß√µes:\")\n",
        "print(f\"A[0,0] + B[0,0] = {A[0,0]} + {B[0,0]} = {C_soma[0,0]}\")\n",
        "print(f\"A[1,2] + B[1,2] = {A[1,2]} + {B[1,2]} = {C_soma[1,2]}\")\n",
        "print(f\"A[2,3] + B[2,3] = {A[2,3]} + {B[2,3]} = {C_soma[2,3]}\")\n",
        "\n",
        "# Testando propriedades\n",
        "print(\"\\nüßÆ Testando propriedades matem√°ticas:\")\n",
        "print(f\"A + B = B + A? {np.array_equal(A + B, B + A)}\")\n",
        "print(\"Comutatividade confirmada! üéâ\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "matrix_subtraction"
      },
      "source": [
        "## ‚ûñ Subtra√ß√£o de Matrizes: A Diferen√ßa que Faz Diferen√ßa\n\nA subtra√ß√£o funciona **igualzinho** √† soma, s√≥ que... subtraindo! üòÑ\n\n### üìê F√≥rmula Matem√°tica\n\n$$D = A - B \\quad \\text{onde} \\quad d_{ij} = a_{ij} - b_{ij}$$\n\n### üéØ Onde Usamos na IA\n\n- **C√°lculo de gradientes**: diferen√ßa entre pesos atuais e anteriores\n- **Fun√ß√£o de perda**: diferen√ßa entre predi√ß√£o e valor real\n- **Regulariza√ß√£o**: penalizando pesos muito grandes\n\n**üî• Dica do Pedro**: Ao contr√°rio da soma, a subtra√ß√£o **N√ÉO √© comutativa**! $A - B \\neq B - A$ (na verdade, $A - B = -(B - A)$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "matrix_subtraction_code"
      },
      "source": [
        "# Agora vamos subtrair!\n",
        "print(\"‚ûñ SUBTRA√á√ÉO DE MATRIZES - Encontrando as Diferen√ßas\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "C_sub = A - B\n",
        "\n",
        "print(\"A - B =\")\n",
        "print(C_sub)\n",
        "\n",
        "print(\"\\nB - A =\")\n",
        "print(B - A)\n",
        "\n",
        "print(\"\\nüîç Verificando que n√£o √© comutativa:\")\n",
        "print(f\"A - B = B - A? {np.array_equal(A - B, B - A)}\")\n",
        "print(f\"Mas A - B = -(B - A)? {np.array_equal(A - B, -(B - A))}\")\n",
        "\n",
        "# Exemplo pr√°tico: calculando \"erro\" entre duas predi√ß√µes\n",
        "print(\"\\nüéØ Exemplo pr√°tico - Diferen√ßa de predi√ß√µes:\")\n",
        "predicao_modelo1 = np.array([[0.8, 0.2], [0.6, 0.4]])\n",
        "predicao_modelo2 = np.array([[0.7, 0.3], [0.5, 0.5]])\n",
        "\n",
        "diferenca = predicao_modelo1 - predicao_modelo2\n",
        "print(\"Diferen√ßa entre modelos:\")\n",
        "print(diferenca)\n",
        "print(f\"\\nDiferen√ßa m√©dia absoluta: {np.abs(diferenca).mean():.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization_operations"
      },
      "source": [
        "## üìä Visualizando as Opera√ß√µes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "visualization_code"
      },
      "source": [
        "# Vamos visualizar nossas opera√ß√µes!\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Matriz A\n",
        "im1 = axes[0,0].imshow(A, cmap='Blues', aspect='auto')\n",
        "axes[0,0].set_title('Matriz A\\n(Pesos da Camada 1)', fontsize=14, fontweight='bold')\n",
        "for i in range(A.shape[0]):\n",
        "    for j in range(A.shape[1]):\n",
        "        axes[0,0].text(j, i, str(A[i,j]), ha='center', va='center', fontweight='bold')\n",
        "\n",
        "# Matriz B\n",
        "im2 = axes[0,1].imshow(B, cmap='Reds', aspect='auto')\n",
        "axes[0,1].set_title('Matriz B\\n(Pesos da Camada 2)', fontsize=14, fontweight='bold')\n",
        "for i in range(B.shape[0]):\n",
        "    for j in range(B.shape[1]):\n",
        "        axes[0,1].text(j, i, str(B[i,j]), ha='center', va='center', fontweight='bold')\n",
        "\n",
        "# Soma A + B\n",
        "im3 = axes[1,0].imshow(A + B, cmap='Greens', aspect='auto')\n",
        "axes[1,0].set_title('A + B\\n(Combina√ß√£o Aditiva)', fontsize=14, fontweight='bold')\n",
        "for i in range(A.shape[0]):\n",
        "    for j in range(A.shape[1]):\n",
        "        axes[1,0].text(j, i, str((A + B)[i,j]), ha='center', va='center', fontweight='bold')\n",
        "\n",
        "# Subtra√ß√£o A - B\n",
        "im4 = axes[1,1].imshow(A - B, cmap='Purples', aspect='auto')\n",
        "axes[1,1].set_title('A - B\\n(Diferen√ßa)', fontsize=14, fontweight='bold')\n",
        "for i in range(A.shape[0]):\n",
        "    for j in range(A.shape[1]):\n",
        "        axes[1,1].text(j, i, str((A - B)[i,j]), ha='center', va='center', fontweight='bold')\n",
        "\n",
        "# Remove ticks para ficar mais limpo\n",
        "for ax in axes.flat:\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('üé® Opera√ß√µes com Matrizes Visualizadas', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Liiindo! Agora voc√™ v√™ como cada opera√ß√£o transforma os dados!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "multiplication_intro"
      },
      "source": [
        "## ‚úñÔ∏è Multiplica√ß√£o de Matrizes: Onde a M√°gica Acontece!\n\nAgora chegamos na **opera√ß√£o mais importante** para redes neurais! A multiplica√ß√£o de matrizes √© onde **a transforma√ß√£o real acontece**.\n\n### ü§Ø Por que √© Diferente?\n\nDiferente da soma e subtra√ß√£o, a multiplica√ß√£o **N√ÉO √© elemento por elemento**. √â bem mais sofisticada!\n\n### üìê A Regra de Ouro\n\nPara multiplicar duas matrizes $A_{m \\times n}$ e $B_{p \\times q}$:\n\n**CONDI√á√ÉO OBRIGAT√ìRIA**: $n = p$ (n√∫mero de colunas de A = n√∫mero de linhas de B)\n\n**RESULTADO**: Matriz $C_{m \\times q}$\n\n### üßÆ A F√≥rmula Matem√°tica\n\n$$c_{ij} = \\sum_{k=1}^{n} a_{ik} \\cdot b_{kj}$$\n\n**Em portugu√™s**: Para calcular o elemento $c_{ij}$, pegamos a **linha i de A** e a **coluna j de B**, multiplicamos **elemento por elemento** e **somamos tudo**!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/algebra-linear-para-ia-modulo-03_img_04.png)\n\n**üî• Dica do Pedro**: Pensa assim - √© como se cada linha da primeira matriz fosse um \"neur√¥nio\" conversando com cada coluna da segunda matriz (que s√£o as \"caracter√≠sticas\")!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "multiplication_example"
      },
      "source": [
        "# Vamos ver a multiplica√ß√£o na pr√°tica!\n",
        "print(\"‚úñÔ∏è MULTIPLICA√á√ÉO DE MATRIZES - A M√°gica das Transforma√ß√µes\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Criando matrizes compat√≠veis para multiplica√ß√£o\n",
        "# A: 3√ó4, B precisa ser 4√óalgo para dar certo\n",
        "X = np.array([[1, 2],\n",
        "              [3, 4],\n",
        "              [5, 6],\n",
        "              [7, 8]])  # 4√ó2\n",
        "\n",
        "print(\"Matriz A (3√ó4):\")\n",
        "print(A)\n",
        "print(f\"\\nMatriz X (4√ó2):\")\n",
        "print(X)\n",
        "\n",
        "# Verificando se podemos multiplicar\n",
        "print(f\"\\nüîç Verifica√ß√£o de compatibilidade:\")\n",
        "print(f\"A.shape = {A.shape} (3√ó4)\")\n",
        "print(f\"X.shape = {X.shape} (4√ó2)\")\n",
        "print(f\"Colunas de A ({A.shape[1]}) = Linhas de X ({X.shape[0]})? {A.shape[1] == X.shape[0]}\")\n",
        "print(\"‚úÖ Podemos multiplicar!\")\n",
        "\n",
        "# Fazendo a multiplica√ß√£o\n",
        "C_mult = A @ X  # Operador @ √© para multiplica√ß√£o de matrizes\n",
        "# Ou tamb√©m: C_mult = np.dot(A, X)\n",
        "\n",
        "print(f\"\\nResultado A √ó X (ser√° {A.shape[0]}√ó{X.shape[1]}):\")\n",
        "print(C_mult)\n",
        "print(f\"Dimens√µes do resultado: {C_mult.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "multiplication_detail"
      },
      "source": [
        "## üîç Entendendo Cada Passo da Multiplica√ß√£o\n\nVamos **abrir a caixa preta** e ver como cada elemento do resultado √© calculado!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "multiplication_step_by_step"
      },
      "source": [
        "# Vamos calcular passo a passo para entender melhor\n",
        "print(\"üîç MULTIPLICA√á√ÉO PASSO A PASSO\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "print(\"Calculando elemento por elemento...\\n\")\n",
        "\n",
        "# Vamos usar matrizes menores para ficar mais claro\n",
        "A_pequena = np.array([[1, 2, 3],\n",
        "                      [4, 5, 6]])\n",
        "\n",
        "B_pequena = np.array([[7, 8],\n",
        "                      [9, 10],\n",
        "                      [11, 12]])\n",
        "\n",
        "print(\"Matriz A (2√ó3):\")\n",
        "print(A_pequena)\n",
        "print(\"\\nMatriz B (3√ó2):\")\n",
        "print(B_pequena)\n",
        "\n",
        "# Calculando cada posi√ß√£o manualmente\n",
        "resultado = np.zeros((2, 2))\n",
        "\n",
        "for i in range(2):  # Para cada linha de A\n",
        "    for j in range(2):  # Para cada coluna de B\n",
        "        # Pegamos linha i de A e coluna j de B\n",
        "        linha_A = A_pequena[i, :]\n",
        "        coluna_B = B_pequena[:, j]\n",
        "        \n",
        "        # Multiplica√ß√£o elemento por elemento e soma\n",
        "        produto = np.sum(linha_A * coluna_B)\n",
        "        resultado[i, j] = produto\n",
        "        \n",
        "        print(f\"\\nC[{i},{j}] = Linha {i} de A √ó Coluna {j} de B\")\n",
        "        print(f\"C[{i},{j}] = {linha_A} √ó {coluna_B}\")\n",
        "        print(f\"C[{i},{j}] = {linha_A[0]}√ó{coluna_B[0]} + {linha_A[1]}√ó{coluna_B[1]} + {linha_A[2]}√ó{coluna_B[2]}\")\n",
        "        print(f\"C[{i},{j}] = {linha_A[0]*coluna_B[0]} + {linha_A[1]*coluna_B[1]} + {linha_A[2]*coluna_B[2]} = {produto}\")\n",
        "\n",
        "print(f\"\\nüéØ Resultado final:\")\n",
        "print(resultado)\n",
        "\n",
        "# Verificando com NumPy\n",
        "resultado_numpy = A_pequena @ B_pequena\n",
        "print(f\"\\n‚úÖ Verifica√ß√£o com NumPy:\")\n",
        "print(resultado_numpy)\n",
        "print(f\"\\nS√£o iguais? {np.allclose(resultado, resultado_numpy)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "order_matters"
      },
      "source": [
        "## ‚ö†Ô∏è Por que a ORDEM Importa? O Drama das Dimens√µes!\n\nAqui est√° o **pulo do gato** que muita gente n√£o entende no come√ßo!\n\n### üö® A Multiplica√ß√£o N√ÉO √© Comutativa!\n\nDiferente da soma, na multiplica√ß√£o: **$A \\times B \\neq B \\times A$**\n\n### üß† Por que isso √© CRUCIAL em Redes Neurais?\n\nImagine voc√™ tem:\n- **Dados de entrada**: 100 amostras √ó 784 caracter√≠sticas (imagens 28√ó28)\n- **Pesos da primeira camada**: 784 entradas √ó 128 neur√¥nios\n\nA **√∫nica ordem que funciona** √©: `Dados @ Pesos`\n\n**Resultado**: 100 amostras √ó 128 neur√¥nios ativados ‚úÖ\n\nSe tentarmos `Pesos @ Dados`: **ERRO!** As dimens√µes n√£o batem! ‚ùå\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/algebra-linear-para-ia-modulo-03_img_05.png)\n\n**üî• Dica do Pedro**: Sempre pense: \"Estou transformando QUEM em O QU√ä?\" A ordem das matrizes conta essa hist√≥ria!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "order_demonstration"
      },
      "source": [
        "# Demonstrando por que a ordem importa!\n",
        "print(\"‚ö†Ô∏è A ORDEM IMPORTA - Demonstra√ß√£o Pr√°tica\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Matrizes de exemplo\n",
        "P = np.array([[1, 2],\n",
        "              [3, 4],\n",
        "              [5, 6]])  # 3√ó2\n",
        "\n",
        "Q = np.array([[7, 8, 9],\n",
        "              [10, 11, 12]])  # 2√ó3\n",
        "\n",
        "print(\"Matriz P (3√ó2):\")\n",
        "print(P)\n",
        "print(\"\\nMatriz Q (2√ó3):\")\n",
        "print(Q)\n",
        "\n",
        "# P √ó Q √© poss√≠vel (3√ó2) √ó (2√ó3) = (3√ó3)\n",
        "PQ = P @ Q\n",
        "print(f\"\\n‚úÖ P √ó Q (poss√≠vel - 3√ó2 √ó 2√ó3 = 3√ó3):\")\n",
        "print(PQ)\n",
        "print(f\"Dimens√µes: {PQ.shape}\")\n",
        "\n",
        "# Q √ó P tamb√©m √© poss√≠vel (2√ó3) √ó (3√ó2) = (2√ó2)\n",
        "QP = Q @ P\n",
        "print(f\"\\n‚úÖ Q √ó P (tamb√©m poss√≠vel - 2√ó3 √ó 3√ó2 = 2√ó2):\")\n",
        "print(QP)\n",
        "print(f\"Dimens√µes: {QP.shape}\")\n",
        "\n",
        "# Mas os resultados s√£o COMPLETAMENTE diferentes!\n",
        "print(f\"\\nü§Ø P√óQ = Q√óP? {np.array_equal(PQ, QP) if PQ.shape == QP.shape else 'Nem as dimens√µes s√£o iguais!'}\")\n",
        "\n",
        "# Exemplo que n√£o funciona\n",
        "print(\"\\n‚ùå Tentativa de multiplica√ß√£o incompat√≠vel:\")\n",
        "R = np.array([[1, 2, 3]])  # 1√ó3\n",
        "S = np.array([[4], [5]])   # 2√ó1\n",
        "\n",
        "print(f\"R (1√ó3): {R}\")\n",
        "print(f\"S (2√ó1): {S.flatten()}\")\n",
        "\n",
        "try:\n",
        "    resultado_impossivel = R @ S\n",
        "    print(\"Resultado:\", resultado_impossivel)\n",
        "except ValueError as e:\n",
        "    print(f\"üí• ERRO: {e}\")\n",
        "    print(\"As dimens√µes n√£o s√£o compat√≠veis para multiplica√ß√£o!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neural_network_example"
      },
      "source": [
        "## üß† Aplica√ß√£o Real: Simulando uma Camada Neural\n\nAgora vamos ver como tudo isso se conecta em uma **rede neural de verdade**!\n\n### üéØ O Cen√°rio\n\nVamos simular:\n- **Entrada**: 5 amostras, cada uma com 4 caracter√≠sticas\n- **Camada densa**: 4 entradas ‚Üí 3 neur√¥nios\n- **Pesos**: matriz 4√ó3\n- **Bias**: vetor com 3 elementos\n\n### üìä A Transforma√ß√£o Completa\n\n$$\\text{Sa√≠da} = \\text{Entrada} \\times \\text{Pesos} + \\text{Bias}$$\n\n$$Y_{5 \\times 3} = X_{5 \\times 4} \\times W_{4 \\times 3} + b_{1 \\times 3}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neural_layer_simulation"
      },
      "source": [
        "# Simulando uma camada neural real!\n",
        "print(\"üß† SIMULA√á√ÉO DE CAMADA NEURAL REAL\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Definindo nossos dados\n",
        "np.random.seed(42)  # Para resultados reproduz√≠veis\n",
        "\n",
        "# Dados de entrada (5 amostras, 4 caracter√≠sticas cada)\n",
        "X = np.random.randn(5, 4)\n",
        "print(\"üìä Dados de entrada X (5 amostras √ó 4 caracter√≠sticas):\")\n",
        "print(np.round(X, 2))\n",
        "print(f\"Dimens√µes: {X.shape}\")\n",
        "\n",
        "# Pesos da camada (4 entradas √ó 3 neur√¥nios)\n",
        "W = np.random.randn(4, 3) * 0.5  # Multiplicamos por 0.5 para pesos menores\n",
        "print(f\"\\n‚öñÔ∏è Matriz de pesos W (4 caracter√≠sticas √ó 3 neur√¥nios):\")\n",
        "print(np.round(W, 2))\n",
        "print(f\"Dimens√µes: {W.shape}\")\n",
        "\n",
        "# Bias (um para cada neur√¥nio)\n",
        "b = np.random.randn(3) * 0.1\n",
        "print(f\"\\nüéØ Vetor bias b (3 neur√¥nios):\")\n",
        "print(np.round(b, 2))\n",
        "print(f\"Dimens√µes: {b.shape}\")\n",
        "\n",
        "# Calculando a sa√≠da da camada\n",
        "print(\"\\nüîÑ Aplicando a transforma√ß√£o linear:\")\n",
        "print(\"Y = X @ W + b\")\n",
        "\n",
        "# Multiplica√ß√£o de matrizes\n",
        "Z = X @ W\n",
        "print(f\"\\nPrimeiro: X @ W (multiplica√ß√£o de matrizes)\")\n",
        "print(f\"Dimens√µes: {X.shape} @ {W.shape} = {Z.shape}\")\n",
        "print(\"Resultado Z =\")\n",
        "print(np.round(Z, 2))\n",
        "\n",
        "# Adi√ß√£o do bias (broadcasting)\n",
        "Y = Z + b\n",
        "print(f\"\\nDepois: Z + b (adi√ß√£o com broadcasting)\")\n",
        "print(\"Sa√≠da final Y =\")\n",
        "print(np.round(Y, 2))\n",
        "print(f\"Dimens√µes finais: {Y.shape}\")\n",
        "\n",
        "print(\"\\nüéâ Liiindo! Acabamos de simular uma camada neural completa!\")\n",
        "print(\"\\nüí° Cada linha de Y representa a ativa√ß√£o dos 3 neur√¥nios para uma amostra\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mermaid_diagram"
      },
      "source": [
        "## üìà Fluxo de Dados em Rede Neural\n\n```mermaid\ngraph LR\n    A[\"Entrada X<br/>(5√ó4)\"] --> B[\"Multiplica√ß√£o<br/>X @ W\"]\n    W[\"Pesos W<br/>(4√ó3)\"] --> B\n    B --> C[\"Resultado Z<br/>(5√ó3)\"]\n    C --> D[\"Adi√ß√£o Bias<br/>Z + b\"]\n    Bias[\"Bias b<br/>(3,)\"] --> D\n    D --> E[\"Sa√≠da Y<br/>(5√ó3)\"]\n    \n    style A fill:#e1f5fe\n    style W fill:#fff3e0\n    style Bias fill:#f3e5f5\n    style E fill:#e8f5e8\n```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "advanced_visualization"
      },
      "source": [
        "# Vamos visualizar o fluxo completo!\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Dados de entrada X\n",
        "im1 = axes[0,0].imshow(X, cmap='Blues', aspect='auto')\n",
        "axes[0,0].set_title('Entrada X\\n(5 amostras √ó 4 caracter√≠sticas)', fontweight='bold')\n",
        "axes[0,0].set_xlabel('Caracter√≠sticas')\n",
        "axes[0,0].set_ylabel('Amostras')\n",
        "\n",
        "# Pesos W\n",
        "im2 = axes[0,1].imshow(W, cmap='Oranges', aspect='auto')\n",
        "axes[0,1].set_title('Pesos W\\n(4 caracter√≠sticas √ó 3 neur√¥nios)', fontweight='bold')\n",
        "axes[0,1].set_xlabel('Neur√¥nios')\n",
        "axes[0,1].set_ylabel('Caracter√≠sticas')\n",
        "\n",
        "# Resultado da multiplica√ß√£o Z\n",
        "im3 = axes[0,2].imshow(Z, cmap='Greens', aspect='auto')\n",
        "axes[0,2].set_title('X @ W = Z\\n(5 amostras √ó 3 neur√¥nios)', fontweight='bold')\n",
        "axes[0,2].set_xlabel('Neur√¥nios')\n",
        "axes[0,2].set_ylabel('Amostras')\n",
        "\n",
        "# Bias\n",
        "axes[1,0].bar(range(len(b)), b, color=['purple', 'magenta', 'violet'])\n",
        "axes[1,0].set_title('Bias b\\n(3 neur√¥nios)', fontweight='bold')\n",
        "axes[1,0].set_xlabel('Neur√¥nios')\n",
        "axes[1,0].set_ylabel('Valor do Bias')\n",
        "\n",
        "# Resultado final Y\n",
        "im4 = axes[1,1].imshow(Y, cmap='viridis', aspect='auto')\n",
        "axes[1,1].set_title('Sa√≠da Final Y = Z + b\\n(5 amostras √ó 3 neur√¥nios)', fontweight='bold')\n",
        "axes[1,1].set_xlabel('Neur√¥nios')\n",
        "axes[1,1].set_ylabel('Amostras')\n",
        "\n",
        "# Compara√ß√£o antes e depois\n",
        "diferenca = Y - Z\n",
        "im5 = axes[1,2].imshow(diferenca, cmap='RdBu', aspect='auto')\n",
        "axes[1,2].set_title('Efeito do Bias\\n(Y - Z)', fontweight='bold')\n",
        "axes[1,2].set_xlabel('Neur√¥nios')\n",
        "axes[1,2].set_ylabel('Amostras')\n",
        "\n",
        "# Adicionando colorbars\n",
        "plt.colorbar(im1, ax=axes[0,0])\n",
        "plt.colorbar(im2, ax=axes[0,1])\n",
        "plt.colorbar(im3, ax=axes[0,2])\n",
        "plt.colorbar(im4, ax=axes[1,1])\n",
        "plt.colorbar(im5, ax=axes[1,2])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('üß† Anatomia Completa de uma Camada Neural', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print(\"üî• Agora voc√™ v√™ EXATAMENTE como os dados fluem pela rede!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "broadcasting"
      },
      "source": [
        "## üì° Broadcasting: O Truque M√°gico do NumPy\n\nVoc√™ reparou como conseguimos somar a matriz Z (5√ó3) com o vetor bias b (3,)? Isso √© **broadcasting**!\n\n### üéØ Como Funciona o Broadcasting\n\nO NumPy \"**estica**\" automaticamente o vetor menor para casar com as dimens√µes da matriz maior:\n\n```\nZ (5√ó3) + b (3,) \n‚Üì\nZ (5√ó3) + b_expandido (5√ó3)  # b √© repetido 5 vezes\n```\n\n### üß† Por que √© Essencial em IA\n\n- **Efici√™ncia**: N√£o precisamos criar matrizes gigantes desnecessariamente\n- **Mem√≥ria**: Economiza RAM preciosa\n- **Simplicidade**: C√≥digo mais limpo e leg√≠vel\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/algebra-linear-para-ia-modulo-03_img_06.png)\n\n**üî• Dica do Pedro**: O broadcasting segue regras espec√≠ficas - as dimens√µes devem ser **compat√≠veis** ou uma delas deve ser **1**!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "broadcasting_examples"
      },
      "source": [
        "# Explorando o broadcasting em detalhes\n",
        "print(\"üì° BROADCASTING - A M√°gica das Dimens√µes\")\n",
        "print(\"=\"*45)\n",
        "\n",
        "# Exemplo 1: Matriz + Vetor\n",
        "matriz = np.array([[1, 2, 3],\n",
        "                   [4, 5, 6],\n",
        "                   [7, 8, 9]])\n",
        "\n",
        "vetor = np.array([10, 20, 30])\n",
        "\n",
        "print(\"Exemplo 1: Matriz (3√ó3) + Vetor (3,)\")\n",
        "print(f\"Matriz:\\n{matriz}\")\n",
        "print(f\"\\nVetor: {vetor}\")\n",
        "\n",
        "resultado1 = matriz + vetor\n",
        "print(f\"\\nResultado (broadcasting autom√°tico):\\n{resultado1}\")\n",
        "\n",
        "# Mostrando o que acontece \"por baixo dos panos\"\n",
        "vetor_expandido = np.tile(vetor, (3, 1))\n",
        "print(f\"\\nO que o NumPy faz internamente (vetor expandido):\\n{vetor_expandido}\")\n",
        "print(f\"\\nVerifica√ß√£o: {np.array_equal(resultado1, matriz + vetor_expandido)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*45)\n",
        "\n",
        "# Exemplo 2: Diferentes formas de broadcasting\n",
        "print(\"Exemplo 2: Diferentes tipos de broadcasting\")\n",
        "\n",
        "A = np.random.randint(1, 10, (2, 3))\n",
        "b_linha = np.random.randint(1, 5, (1, 3))  # Vetor linha\n",
        "b_coluna = np.random.randint(1, 5, (2, 1))  # Vetor coluna\n",
        "escalar = 5\n",
        "\n",
        "print(f\"Matriz A (2√ó3):\\n{A}\")\n",
        "print(f\"\\nVetor linha b_linha (1√ó3): {b_linha}\")\n",
        "print(f\"Vetor coluna b_coluna (2√ó1):\\n{b_coluna}\")\n",
        "print(f\"Escalar: {escalar}\")\n",
        "\n",
        "print(f\"\\nA + b_linha (broadcasting em linhas):\\n{A + b_linha}\")\n",
        "print(f\"\\nA + b_coluna (broadcasting em colunas):\\n{A + b_coluna}\")\n",
        "print(f\"\\nA + escalar (broadcasting total):\\n{A + escalar}\")\n",
        "\n",
        "print(\"\\nüéØ Todos esses s√£o exemplos de broadcasting em a√ß√£o!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_1"
      },
      "source": [
        "## üí™ Exerc√≠cio 1: Construindo sua Primeira Rede\n\nAgora √© sua vez! Vamos implementar uma **mini-rede neural** com 2 camadas:\n\n### üéØ Especifica√ß√µes:\n- **Entrada**: 10 amostras, 5 caracter√≠sticas cada\n- **Camada 1**: 5 ‚Üí 8 neur√¥nios\n- **Camada 2**: 8 ‚Üí 3 neur√¥nios (sa√≠da final)\n- Use **fun√ß√£o ReLU** entre as camadas: $\\text{ReLU}(x) = \\max(0, x)$\n\n### üìã Sua Miss√£o:\n1. Crie as matrizes de pesos e vetores bias\n2. Implemente o forward pass completo\n3. Visualize os resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exercise_1_solution"
      },
      "source": [
        "# üí™ EXERC√çCIO 1 - Sua vez de brilhar!\n",
        "print(\"üí™ EXERC√çCIO 1: Construindo uma Mini-Rede Neural\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Definindo a seed para resultados consistentes\n",
        "np.random.seed(123)\n",
        "\n",
        "# TODO: Crie os dados de entrada (10 amostras, 5 caracter√≠sticas)\n",
        "X_input = np.random.randn(10, 5)\n",
        "print(f\"üìä Dados de entrada: {X_input.shape}\")\n",
        "\n",
        "# TODO: Crie os pesos e bias da primeira camada (5 ‚Üí 8)\n",
        "W1 = np.random.randn(5, 8) * 0.3  # Pesos pequenos s√£o melhores\n",
        "b1 = np.random.randn(8) * 0.1\n",
        "print(f\"‚öñÔ∏è Camada 1 - W1: {W1.shape}, b1: {b1.shape}\")\n",
        "\n",
        "# TODO: Crie os pesos e bias da segunda camada (8 ‚Üí 3)\n",
        "W2 = np.random.randn(8, 3) * 0.3\n",
        "b2 = np.random.randn(3) * 0.1\n",
        "print(f\"‚öñÔ∏è Camada 2 - W2: {W2.shape}, b2: {b2.shape}\")\n",
        "\n",
        "# Implementando o forward pass\n",
        "print(\"\\nüîÑ Executando Forward Pass:\")\n",
        "\n",
        "# Primeira camada\n",
        "Z1 = X_input @ W1 + b1\n",
        "A1 = np.maximum(0, Z1)  # ReLU activation\n",
        "print(f\"Camada 1: {X_input.shape} ‚Üí {Z1.shape} ‚Üí {A1.shape} (ap√≥s ReLU)\")\n",
        "\n",
        "# Segunda camada\n",
        "Z2 = A1 @ W2 + b2\n",
        "A2 = Z2  # Sa√≠da linear (sem ativa√ß√£o)\n",
        "print(f\"Camada 2: {A1.shape} ‚Üí {Z2.shape} ‚Üí {A2.shape} (sa√≠da final)\")\n",
        "\n",
        "print(f\"\\nüéâ Parab√©ns! Voc√™ criou sua primeira rede neural!\")\n",
        "print(f\"Entrada: {X_input.shape} ‚Üí Sa√≠da: {A2.shape}\")\n",
        "print(f\"\\nPrimeiras 3 predi√ß√µes:\")\n",
        "print(np.round(A2[:3], 3))\n",
        "\n",
        "# Verificando quantos neur√¥nios foram ativados na camada oculta\n",
        "neurons_ativos = np.sum(A1 > 0, axis=1)\n",
        "print(f\"\\nüß† Neur√¥nios ativados por amostra (de 8 poss√≠veis):\")\n",
        "print(neurons_ativos)\n",
        "print(f\"M√©dia de ativa√ß√£o: {neurons_ativos.mean():.1f} neur√¥nios por amostra\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_2"
      },
      "source": [
        "## üî• Exerc√≠cio 2: Explorando o Efeito da Ordem\n\nVamos investigar **experimentalmente** como a ordem das opera√ß√µes afeta os resultados!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exercise_2_solution"
      },
      "source": [
        "# üî• EXERC√çCIO 2 - Investigando a Ordem das Opera√ß√µes\n",
        "print(\"üî• EXERC√çCIO 2: O Mist√©rio da Ordem\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Cen√°rio: Duas transforma√ß√µes sequenciais\n",
        "np.random.seed(456)\n",
        "\n",
        "# Dados iniciais\n",
        "dados = np.random.randn(4, 3)\n",
        "transf_A = np.random.randn(3, 5)\n",
        "transf_B = np.random.randn(5, 2)\n",
        "\n",
        "print(\"üéØ Cen√°rio: Duas transforma√ß√µes em sequ√™ncia\")\n",
        "print(f\"Dados: {dados.shape}\")\n",
        "print(f\"Transforma√ß√£o A: {transf_A.shape}\")\n",
        "print(f\"Transforma√ß√£o B: {transf_B.shape}\")\n",
        "\n",
        "# M√©todo 1: Aplicar transforma√ß√µes uma por vez\n",
        "print(\"\\nüìä M√©todo 1: Passo a passo\")\n",
        "resultado_passo1 = dados @ transf_A\n",
        "resultado_final1 = resultado_passo1 @ transf_B\n",
        "print(f\"(Dados @ A) @ B = {dados.shape} ‚Üí {resultado_passo1.shape} ‚Üí {resultado_final1.shape}\")\n",
        "\n",
        "# M√©todo 2: Pr√©-computar a transforma√ß√£o combinada\n",
        "print(\"\\n‚ö° M√©todo 2: Transforma√ß√£o combinada\")\n",
        "transf_combinada = transf_A @ transf_B\n",
        "resultado_final2 = dados @ transf_combinada\n",
        "print(f\"Dados @ (A @ B) = {dados.shape} ‚Üí {resultado_final2.shape}\")\n",
        "print(f\"Transforma√ß√£o combinada: {transf_combinada.shape}\")\n",
        "\n",
        "# Verificando se s√£o iguais\n",
        "print(f\"\\nüîç Os resultados s√£o iguais? {np.allclose(resultado_final1, resultado_final2)}\")\n",
        "print(\"\\nüí° Isso demonstra a ASSOCIATIVIDADE da multiplica√ß√£o de matrizes!\")\n",
        "print(\"(A @ B) @ C = A @ (B @ C)\")\n",
        "\n",
        "# Medindo performance\n",
        "import time\n",
        "\n",
        "# Testando com dados maiores\n",
        "dados_grandes = np.random.randn(1000, 100)\n",
        "A_grande = np.random.randn(100, 50)\n",
        "B_grande = np.random.randn(50, 10)\n",
        "\n",
        "# M√©todo passo a passo\n",
        "start_time = time.time()\n",
        "for _ in range(100):\n",
        "    temp = dados_grandes @ A_grande\n",
        "    result1 = temp @ B_grande\n",
        "time_metodo1 = time.time() - start_time\n",
        "\n",
        "# M√©todo combinado\n",
        "AB_combined = A_grande @ B_grande\n",
        "start_time = time.time()\n",
        "for _ in range(100):\n",
        "    result2 = dados_grandes @ AB_combined\n",
        "time_metodo2 = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è Performance (100 repeti√ß√µes):\")\n",
        "print(f\"M√©todo passo a passo: {time_metodo1:.4f}s\")\n",
        "print(f\"M√©todo combinado: {time_metodo2:.4f}s\")\n",
        "print(f\"Speedup: {time_metodo1/time_metodo2:.2f}x mais r√°pido!\")\n",
        "\n",
        "print(\"\\nüöÄ Conclus√£o: A ordem pode afetar a EFICI√äNCIA, n√£o apenas o resultado!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "real_world_connection"
      },
      "source": [
        "## üåç Conex√µes com o Mundo Real\n\nAgora que dominamos as opera√ß√µes com matrizes, vamos ver onde elas aparecem **no mundo real da IA**:\n\n### üñºÔ∏è Processamento de Imagens\n- **Convolu√ß√£o**: Multiplica√ß√£o de matrizes para detectar padr√µes\n- **Pooling**: Redu√ß√£o de dimensionalidade\n- **Transforma√ß√µes geom√©tricas**: Rota√ß√£o, escala, transla√ß√£o\n\n### üó£Ô∏è Processamento de Linguagem Natural\n- **Embeddings**: Matriz palavra √ó caracter√≠sticas\n- **Attention**: Multiplica√ß√£o para calcular relev√¢ncia\n- **Transformers**: Camadas densas com multiplica√ß√£o matricial\n\n### üéØ Sistemas de Recomenda√ß√£o\n- **Matriz usu√°rio √ó item**: Collaborative filtering\n- **Fatora√ß√£o matricial**: SVD para descobrir padr√µes latentes\n- **Similaridade**: Produto escalar entre vetores\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/algebra-linear-para-ia-modulo-03_img_07.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "real_world_examples"
      },
      "source": [
        "# Exemplos do mundo real com nossas opera√ß√µes\n",
        "print(\"üåç APLICA√á√ïES NO MUNDO REAL\")\n",
        "print(\"=\"*35)\n",
        "\n",
        "# Simula√ß√£o 1: Sistema de Recomenda√ß√£o Simples\n",
        "print(\"üé¨ Simula√ß√£o: Sistema de Recomenda√ß√£o de Filmes\")\n",
        "\n",
        "# Matriz usu√°rio x caracter√≠sticas dos filmes\n",
        "# Caracter√≠sticas: [A√ß√£o, Com√©dia, Drama, Fic√ß√£o, Romance]\n",
        "preferencias_usuarios = np.array([\n",
        "    [5, 2, 3, 4, 1],  # Usu√°rio 1: ama a√ß√£o\n",
        "    [1, 5, 4, 2, 5],  # Usu√°rio 2: ama com√©dia e romance\n",
        "    [3, 3, 5, 3, 4],  # Usu√°rio 3: ama drama\n",
        "    [4, 1, 2, 5, 2]   # Usu√°rio 4: ama a√ß√£o e fic√ß√£o\n",
        "])\n",
        "\n",
        "# Caracter√≠sticas dos filmes dispon√≠veis\n",
        "filmes_caracteristicas = np.array([\n",
        "    [4, 1, 2, 3, 1],  # Filme A: A√ß√£o/Fic√ß√£o\n",
        "    [1, 5, 3, 1, 4],  # Filme B: Com√©dia/Romance\n",
        "    [2, 2, 5, 2, 3],  # Filme C: Drama\n",
        "    [5, 1, 1, 4, 1],  # Filme D: A√ß√£o/Fic√ß√£o pura\n",
        "    [1, 4, 4, 1, 5]   # Filme E: Com√©dia/Romance/Drama\n",
        "]).T  # Transposta para ficar (caracter√≠sticas √ó filmes)\n",
        "\n",
        "# Calculando scores de recomenda√ß√£o\n",
        "scores_recomendacao = preferencias_usuarios @ filmes_caracteristicas\n",
        "\n",
        "print(f\"Prefer√™ncias dos usu√°rios (4 usu√°rios √ó 5 caracter√≠sticas):\")\n",
        "print(preferencias_usuarios)\n",
        "print(f\"\\nCaracter√≠sticas dos filmes (5 caracter√≠sticas √ó 5 filmes):\")\n",
        "print(filmes_caracteristicas)\n",
        "print(f\"\\nScores de recomenda√ß√£o (4 usu√°rios √ó 5 filmes):\")\n",
        "print(scores_recomendacao)\n",
        "\n",
        "# Encontrando melhores recomenda√ß√µes para cada usu√°rio\n",
        "filmes_nomes = ['A√ß√£o A', 'Com√©dia B', 'Drama C', 'A√ß√£o D', 'Romance E']\n",
        "print(\"\\nüèÜ Melhores recomenda√ß√µes por usu√°rio:\")\n",
        "for i in range(4):\n",
        "    melhor_filme_idx = np.argmax(scores_recomendacao[i])\n",
        "    melhor_score = scores_recomendacao[i, melhor_filme_idx]\n",
        "    print(f\"Usu√°rio {i+1}: {filmes_nomes[melhor_filme_idx]} (score: {melhor_score})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Simula√ß√£o 2: Transforma√ß√£o de Embeddings\n",
        "print(\"üî§ Simula√ß√£o: Transforma√ß√£o de Word Embeddings\")\n",
        "\n",
        "# Embeddings de palavras (5 palavras √ó 4 dimens√µes)\n",
        "word_embeddings = np.random.randn(5, 4)\n",
        "palavras = ['gato', 'cachorro', 'peixe', 'p√°ssaro', 'cobra']\n",
        "\n",
        "# Matriz de transforma√ß√£o para novo espa√ßo sem√¢ntico\n",
        "transformacao_semantica = np.random.randn(4, 3)\n",
        "\n",
        "# Aplicando transforma√ß√£o\n",
        "novos_embeddings = word_embeddings @ transformacao_semantica\n",
        "\n",
        "print(f\"Embeddings originais ({word_embeddings.shape}):\")\n",
        "for i, palavra in enumerate(palavras):\n",
        "    print(f\"{palavra:8}: {np.round(word_embeddings[i], 2)}\")\n",
        "\n",
        "print(f\"\\nNovos embeddings transformados ({novos_embeddings.shape}):\")\n",
        "for i, palavra in enumerate(palavras):\n",
        "    print(f\"{palavra:8}: {np.round(novos_embeddings[i], 2)}\")\n",
        "\n",
        "print(\"\\nüéØ Isso √© exatamente como funciona uma camada densa em NLP!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "performance_tips"
      },
      "source": [
        "## ‚ö° Dicas de Performance e Boas Pr√°ticas\n\nAntes de finalizarmos, aqui est√£o as **dicas de ouro** para trabalhar com matrizes de forma eficiente:\n\n### üöÄ Otimiza√ß√µes de Performance\n\n1. **Use NumPy**: Sempre prefira opera√ß√µes vetorizadas\n2. **Evite loops**: Uma multiplica√ß√£o matricial √© milhares de vezes mais r√°pida que loops aninhados\n3. **Ordem importa**: `A @ B @ C` pode ser calculado como `(A @ B) @ C` ou `A @ (B @ C)` - escolha a ordem mais eficiente\n4. **Broadcasting**: Use quando poss√≠vel para economizar mem√≥ria\n\n### üéØ Dicas de Debug\n\n1. **Sempre verifique dimens√µes** antes de opera√ß√µes\n2. **Use asserts** para validar shapes\n3. **Visualize** matrizes pequenas para entender o que est√° acontecendo\n4. **Teste com dados sint√©ticos** antes de usar dados reais\n\n**üî• Dica Final do Pedro**: Quando em d√∫vida sobre dimens√µes, desenhe no papel! S√©rio, isso funciona! ‚úèÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "performance_comparison"
      },
      "source": [
        "# Compara√ß√£o de performance: NumPy vs Loops puros\n",
        "print(\"‚ö° COMPARA√á√ÉO DE PERFORMANCE\")\n",
        "print(\"=\"*35)\n",
        "\n",
        "import time\n",
        "\n",
        "# Criando matrizes de teste\n",
        "size = 200\n",
        "A_test = np.random.randn(size, size)\n",
        "B_test = np.random.randn(size, size)\n",
        "\n",
        "print(f\"Testando multiplica√ß√£o de matrizes {size}√ó{size}\")\n",
        "\n",
        "# M√©todo 1: NumPy otimizado\n",
        "start_time = time.time()\n",
        "result_numpy = A_test @ B_test\n",
        "time_numpy = time.time() - start_time\n",
        "\n",
        "print(f\"\\nüöÄ NumPy (otimizado): {time_numpy:.4f} segundos\")\n",
        "\n",
        "# M√©todo 2: Loops puros (s√≥ para matrizes pequenas!)\n",
        "if size <= 50:  # S√≥ testamos com matrizes pequenas\n",
        "    A_small = A_test[:50, :50]\n",
        "    B_small = B_test[:50, :50]\n",
        "    \n",
        "    start_time = time.time()\n",
        "    result_loops = np.zeros((50, 50))\n",
        "    for i in range(50):\n",
        "        for j in range(50):\n",
        "            for k in range(50):\n",
        "                result_loops[i, j] += A_small[i, k] * B_small[k, j]\n",
        "    time_loops = time.time() - start_time\n",
        "    \n",
        "    print(f\"üêå Loops puros (50√ó50): {time_loops:.4f} segundos\")\n",
        "    print(f\"üéØ NumPy √© {time_loops/time_numpy:.0f}x mais r√°pido!\")\n",
        "else:\n",
        "    print(\"üêå Loops puros: Muito lento para testar com matrizes grandes!\")\n",
        "\n",
        "# Testando diferentes ordens de multiplica√ß√£o\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"üßÆ TESTANDO ORDEM DE MULTIPLICA√á√ÉO\")\n",
        "\n",
        "# Cen√°rio: A(100√ó500) @ B(500√ó50) @ C(50√ó10)\n",
        "A_ordem = np.random.randn(100, 500)\n",
        "B_ordem = np.random.randn(500, 50)\n",
        "C_ordem = np.random.randn(50, 10)\n",
        "\n",
        "# Ordem 1: (A @ B) @ C\n",
        "start_time = time.time()\n",
        "AB = A_ordem @ B_ordem  # 100√ó50\n",
        "result_ordem1 = AB @ C_ordem  # 100√ó10\n",
        "time_ordem1 = time.time() - start_time\n",
        "\n",
        "# Ordem 2: A @ (B @ C)\n",
        "start_time = time.time()\n",
        "BC = B_ordem @ C_ordem  # 500√ó10\n",
        "result_ordem2 = A_ordem @ BC  # 100√ó10\n",
        "time_ordem2 = time.time() - start_time\n",
        "\n",
        "print(f\"Ordem 1 - (A @ B) @ C: {time_ordem1:.6f}s\")\n",
        "print(f\"Ordem 2 - A @ (B @ C): {time_ordem2:.6f}s\")\n",
        "print(f\"Resultados iguais? {np.allclose(result_ordem1, result_ordem2)}\")\n",
        "\n",
        "if time_ordem1 < time_ordem2:\n",
        "    print(f\"üèÜ Ordem 1 √© {time_ordem2/time_ordem1:.2f}x mais r√°pida!\")\n",
        "else:\n",
        "    print(f\"üèÜ Ordem 2 √© {time_ordem1/time_ordem2:.2f}x mais r√°pida!\")\n",
        "\n",
        "print(\"\\nüí° A ordem das opera√ß√µes pode fazer MUITA diferen√ßa na performance!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## üéØ Resum√£o: O que Aprendemos Hoje\n\nParab√©ns! üéâ Voc√™ acabou de dominar as **opera√ß√µes fundamentais com matrizes**! Vamos recapitular:\n\n### ‚úÖ Conceitos Dominados\n\n1. **Soma e Subtra√ß√£o**: Opera√ß√µes elemento por elemento\n   - Requer matrizes de **mesmas dimens√µes**\n   - Soma √© **comutativa**, subtra√ß√£o **n√£o √©**\n\n2. **Multiplica√ß√£o de Matrizes**: A estrela do show!\n   - **Regra de ouro**: Colunas de A = Linhas de B\n   - **N√ÉO √© comutativa**: $A \\times B \\neq B \\times A$\n   - **√â associativa**: $(A \\times B) \\times C = A \\times (B \\times C)$\n\n3. **Broadcasting**: A m√°gica do NumPy\n   - Permite opera√ß√µes entre arrays de dimens√µes diferentes\n   - Essencial para adicionar bias em redes neurais\n\n### üß† Conex√£o com Redes Neurais\n\n- **Cada camada** √© uma multiplica√ß√£o matricial seguida de soma (bias)\n- **A ordem importa**: determina o fluxo de informa√ß√£o\n- **Dimens√µes contam a hist√≥ria**: entrada ‚Üí transforma√ß√£o ‚Üí sa√≠da\n\n### üöÄ Prepara√ß√£o para os Pr√≥ximos M√≥dulos\n\nNo **M√≥dulo 4**, vamos mergulhar fundo no **NumPy** e ver todas essas opera√ß√µes na pr√°tica com dados reais!\n\nNos m√≥dulos seguintes, usaremos essas bases para:\n- **Resolver sistemas lineares** (M√≥dulo 5)\n- **Entender transforma√ß√µes lineares** (M√≥dulo 6)\n- **Trabalhar com inversas e transpostas** (M√≥dulo 7)\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/algebra-linear-para-ia-modulo-03_img_08.png)\n\n### üî• √öltima Dica do Pedro\n\n> **\"Matrizes s√£o a linguagem universal da IA. Agora voc√™ fala fluentemente!\"** \n>\n> Continue praticando, visualizando e **sempre** verificando as dimens√µes. A matem√°tica √© linda quando voc√™ entende o que est√° acontecendo por tr√°s dos n√∫meros! ‚ú®\n\n---\n\n**Nos vemos no pr√≥ximo m√≥dulo! Bora dominar o NumPy! üêçüìä**"
      ]
    }
  ]
}