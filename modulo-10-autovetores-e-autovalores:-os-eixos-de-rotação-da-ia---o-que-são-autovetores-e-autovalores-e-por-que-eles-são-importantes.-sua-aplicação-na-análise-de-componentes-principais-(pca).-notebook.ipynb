{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Autovetores e Autovalores: Os Eixos de Rota√ß√£o da IA\n\n## M√≥dulo 10 - √Ålgebra Linear para IA\n\n### Pedro Nunes Guth\n\n---\n\nBora pro grande final do nosso curso! üöÄ\n\nT√°, mas o que s√£o esses **autovetores** e **autovalores**? E por que eles s√£o t√£o importantes pra IA?\n\nImagina que voc√™ t√° dirigindo na estrada e precisa fazer uma curva. Tem dire√ß√µes que quando voc√™ aplica for√ßa no volante, o carro simplesmente \"estica\" naquela dire√ß√£o sem mudar a orienta√ß√£o - esses s√£o os **autovetores**! E a quantidade de \"esticamento\" √© o **autovalor**.\n\nNa IA, esses caras s√£o os **eixos principais** dos nossos dados. Eles nos mostram as dire√ß√µes mais importantes da informa√ß√£o!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/algebra-linear-para-ia-modulo-10_img_01.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - Nossos companheiros de jornada! üõ†Ô∏è\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_blobs, load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Configura√ß√£o dos gr√°ficos\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"üìö Bibliotecas carregadas! Bora mergulhar nos autovetores!\")\n",
        "print(f\"NumPy vers√£o: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé≠ O Conceito Fundamental: O que S√£o Autovetores?\n\nVamos come√ßar com uma analogia brasileira cl√°ssica!\n\nImagina o **Carnaval do Rio** - voc√™ t√° na Sapuca√≠ e v√™ milhares de pessoas se movendo. Tem gente indo pra todo lado, mas existem algumas **dire√ß√µes principais** do movimento:\n\n1. üìç **Dire√ß√£o da Passarela**: Todo mundo se move principalmente nessa dire√ß√£o\n2. üìç **Dire√ß√£o das Arquibancadas**: Movimento secund√°rio, mas importante\n\nOs **autovetores** s√£o exatamente essas dire√ß√µes principais! E os **autovalores** nos dizem o quanto de movimento acontece em cada dire√ß√£o.\n\n### üßÆ Defini√ß√£o Matem√°tica\n\nPara uma matriz quadrada $A$, um vetor $\\mathbf{v}$ √© um **autovetor** se:\n\n$$A\\mathbf{v} = \\lambda\\mathbf{v}$$\n\nOnde:\n- $\\mathbf{v}$ √© o **autovetor** (dire√ß√£o que n√£o muda)\n- $\\lambda$ √© o **autovalor** (fator de escalonamento)\n- $A$ √© nossa matriz de transforma√ß√£o\n\n**Traduzindo**: Quando aplicamos a matriz $A$ no vetor $\\mathbf{v}$, ele n√£o muda de dire√ß√£o, apenas de tamanho!\n\n**üéØ Dica do Pedro**: Pensa assim - √© como se o autovetor fosse uma mangueira de jardim apontada numa dire√ß√£o fixa, e o autovalor fosse a press√£o da √°gua!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um exemplo visual simples! üé®\n",
        "\n",
        "# Matriz exemplo 2x2\n",
        "A = np.array([[3, 1],\n",
        "              [0, 2]])\n",
        "\n",
        "print(\"üéØ Nossa matriz de transforma√ß√£o A:\")\n",
        "print(A)\n",
        "print()\n",
        "\n",
        "# Calculando autovetores e autovalores\n",
        "autovalores, autovetores = np.linalg.eig(A)\n",
        "\n",
        "print(\"üî• Autovalores (Œª):\")\n",
        "for i, val in enumerate(autovalores):\n",
        "    print(f\"Œª{i+1} = {val:.3f}\")\n",
        "\n",
        "print(\"\\nüéØ Autovetores (v):\")\n",
        "for i, vec in enumerate(autovetores.T):\n",
        "    print(f\"v{i+1} = [{vec[0]:.3f}, {vec[1]:.3f}]\")\n",
        "\n",
        "print(\"\\n‚úÖ Verifica√ß√£o da equa√ß√£o Av = Œªv:\")\n",
        "for i in range(len(autovalores)):\n",
        "    v = autovetores[:, i]\n",
        "    Av = A @ v\n",
        "    lambda_v = autovalores[i] * v\n",
        "    \n",
        "    print(f\"\\nAutovetor {i+1}:\")\n",
        "    print(f\"A*v = [{Av[0]:.3f}, {Av[1]:.3f}]\")\n",
        "    print(f\"Œª*v = [{lambda_v[0]:.3f}, {lambda_v[1]:.3f}]\")\n",
        "    print(f\"S√£o iguais? {np.allclose(Av, lambda_v)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® Visualizando a Magia dos Autovetores\n\nAgora vamos ver isso visualmente! Lembra das **transforma√ß√µes lineares** do M√≥dulo 6? Os autovetores s√£o as dire√ß√µes que **n√£o mudam** quando aplicamos a transforma√ß√£o!\n\n√â como se fossem as **\"estradas fixas\"** no meio de um terremoto - tudo ao redor se move, mas elas mant√™m a dire√ß√£o!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√£o √©pica dos autovetores! üé≠\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Vetores originais (antes da transforma√ß√£o)\n",
        "vetores_originais = np.array([[1, 0], [0, 1], [1, 1], [-1, 1]])\n",
        "cores = ['red', 'blue', 'green', 'orange']\n",
        "labels = ['[1,0]', '[0,1]', '[1,1]', '[-1,1]']\n",
        "\n",
        "# Plot 1: Antes da transforma√ß√£o\n",
        "ax1.set_title('üéØ Antes da Transforma√ß√£o A', fontsize=14, fontweight='bold')\n",
        "for i, (vec, cor, label) in enumerate(zip(vetores_originais, cores, labels)):\n",
        "    ax1.arrow(0, 0, vec[0], vec[1], head_width=0.1, head_length=0.1, \n",
        "              fc=cor, ec=cor, linewidth=2, label=f'v{i+1} = {label}')\n",
        "\n",
        "# Adicionando os autovetores\n",
        "for i, vec in enumerate(autovetores.T):\n",
        "    ax1.arrow(0, 0, vec[0], vec[1], head_width=0.15, head_length=0.15,\n",
        "              fc='black', ec='black', linewidth=3, linestyle='--',\n",
        "              label=f'Autovetor {i+1}')\n",
        "\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_xlim(-2, 2)\n",
        "ax1.set_ylim(-2, 2)\n",
        "ax1.legend()\n",
        "ax1.set_aspect('equal')\n",
        "\n",
        "# Plot 2: Depois da transforma√ß√£o\n",
        "ax2.set_title('üöÄ Depois da Transforma√ß√£o A', fontsize=14, fontweight='bold')\n",
        "for i, (vec, cor, label) in enumerate(zip(vetores_originais, cores, labels)):\n",
        "    vec_transformado = A @ vec\n",
        "    ax2.arrow(0, 0, vec_transformado[0], vec_transformado[1], \n",
        "              head_width=0.1, head_length=0.1, fc=cor, ec=cor, linewidth=2,\n",
        "              label=f'A*v{i+1}')\n",
        "\n",
        "# Autovetores transformados (s√≥ mudam de tamanho!)\n",
        "for i, (vec, val) in enumerate(zip(autovetores.T, autovalores)):\n",
        "    vec_transformado = A @ vec\n",
        "    ax2.arrow(0, 0, vec_transformado[0], vec_transformado[1], \n",
        "              head_width=0.15, head_length=0.15, fc='black', ec='black', \n",
        "              linewidth=3, linestyle='--', label=f'A*Autovetor {i+1} (Œª={val:.1f})')\n",
        "\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_xlim(-4, 4)\n",
        "ax2.set_ylim(-4, 4)\n",
        "ax2.legend()\n",
        "ax2.set_aspect('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéâ Olha s√≥ que liiindo! Os autovetores (linhas tracejadas pretas) mantiveram a dire√ß√£o!\")\n",
        "print(\"üìç S√≥ mudaram de tamanho pelos autovalores correspondentes!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Por Que Autovetores S√£o Cruciais na IA?\n\nT√°, mas por que diabos isso √© importante pra IA? Excelente pergunta! ü§î\n\nOs autovetores nos ajudam a encontrar as **dire√ß√µes mais importantes** nos nossos dados. √â como se fosse um GPS que nos mostra:\n\n### üéØ 1. **Redu√ß√£o de Dimensionalidade**\n- Pega 1000 features e reduz para 50 mais importantes\n- Mant√©m a informa√ß√£o essencial, elimina o \"ru√≠do\"\n\n### üéØ 2. **Compress√£o de Dados**\n- Imagens, √°udio, texto - todos podem ser comprimidos\n- Lembra da **SVD do M√≥dulo 9**? Os autovetores est√£o l√°!\n\n### üéØ 3. **An√°lise de Padr√µes**\n- Encontra as \"tend√™ncias principais\" nos dados\n- Como as pessoas se comportam, que produtos vendem mais, etc.\n\n### üîÑ Diagrama do Fluxo de Informa√ß√£o\n\n```mermaid\ngraph TD\n    A[Dados Originais<br/>Alta Dimensionalidade] --> B[C√°lculo da Matriz<br/>de Covari√¢ncia]\n    B --> C[Autovetores e<br/>Autovalores]\n    C --> D[Ordena√ß√£o por<br/>Import√¢ncia]\n    D --> E[Sele√ß√£o dos<br/>Componentes Principais]\n    E --> F[Dados Reduzidos<br/>Mantendo Informa√ß√£o]\n```\n\n**üéØ Dica do Pedro**: √â como escolher as melhores fotos de uma viagem - voc√™ quer manter as que capturam a \"ess√™ncia\" da experi√™ncia!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar dados sint√©ticos para entender melhor! üé≤\n",
        "\n",
        "# Gerando dados com correla√ß√£o (lembra das matrizes de covari√¢ncia?)\n",
        "np.random.seed(42)\n",
        "n_pontos = 200\n",
        "\n",
        "# Dados correlacionados - como altura e peso\n",
        "altura = np.random.normal(170, 10, n_pontos)  # cm\n",
        "peso = 0.8 * altura + np.random.normal(0, 5, n_pontos) - 100  # kg\n",
        "\n",
        "# Criando nossa matriz de dados\n",
        "dados = np.column_stack([altura, peso])\n",
        "\n",
        "print(\"üìä Dados sint√©ticos criados: Altura vs Peso\")\n",
        "print(f\"Shape dos dados: {dados.shape}\")\n",
        "print(f\"\\nM√©dia - Altura: {np.mean(altura):.1f}cm, Peso: {np.mean(peso):.1f}kg\")\n",
        "print(f\"Desvio - Altura: {np.std(altura):.1f}cm, Peso: {np.std(peso):.1f}kg\")\n",
        "\n",
        "# Centralizando os dados (importante para PCA!)\n",
        "dados_centralizados = dados - np.mean(dados, axis=0)\n",
        "\n",
        "# Calculando matriz de covari√¢ncia\n",
        "cov_matrix = np.cov(dados_centralizados.T)\n",
        "print(\"\\nüéØ Matriz de Covari√¢ncia:\")\n",
        "print(cov_matrix)\n",
        "\n",
        "# Autovetores e autovalores da matriz de covari√¢ncia\n",
        "autovalores_cov, autovetores_cov = np.linalg.eig(cov_matrix)\n",
        "\n",
        "print(\"\\nüî• Autovalores da Covari√¢ncia:\")\n",
        "for i, val in enumerate(autovalores_cov):\n",
        "    variancia_explicada = val / np.sum(autovalores_cov) * 100\n",
        "    print(f\"PC{i+1}: {val:.2f} ({variancia_explicada:.1f}% da vari√¢ncia)\")\n",
        "\n",
        "print(\"\\nüéØ Autovetores (Componentes Principais):\")\n",
        "for i, vec in enumerate(autovetores_cov.T):\n",
        "    print(f\"PC{i+1}: [{vec[0]:.3f}, {vec[1]:.3f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√£o dos Componentes Principais! üé®\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Scatter plot dos dados\n",
        "ax.scatter(altura, peso, alpha=0.6, color='skyblue', s=50, label='Dados Originais')\n",
        "\n",
        "# Centro dos dados\n",
        "centro_x, centro_y = np.mean(altura), np.mean(peso)\n",
        "ax.scatter(centro_x, centro_y, color='red', s=100, marker='x', linewidth=3, label='Centro')\n",
        "\n",
        "# Plotando os autovetores (componentes principais)\n",
        "escala = 30  # Para visualiza√ß√£o\n",
        "\n",
        "for i, (vec, val) in enumerate(zip(autovetores_cov.T, autovalores_cov)):\n",
        "    # Autovetor escalado pelo autovalor\n",
        "    vec_escalado = vec * np.sqrt(val) * escala\n",
        "    \n",
        "    ax.arrow(centro_x, centro_y, vec_escalado[0], vec_escalado[1],\n",
        "             head_width=2, head_length=2, fc=f'C{i}', ec=f'C{i}', \n",
        "             linewidth=4, label=f'PC{i+1} (Œª={val:.0f})')\n",
        "    \n",
        "    ax.arrow(centro_x, centro_y, -vec_escalado[0], -vec_escalado[1],\n",
        "             head_width=2, head_length=2, fc=f'C{i}', ec=f'C{i}', \n",
        "             linewidth=4)\n",
        "\n",
        "ax.set_xlabel('Altura (cm)', fontsize=14)\n",
        "ax.set_ylabel('Peso (kg)', fontsize=14)\n",
        "ax.set_title('üéØ Componentes Principais: As Dire√ß√µes Mais Importantes!', \n",
        "             fontsize=16, fontweight='bold')\n",
        "ax.legend(fontsize=12)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéâ Liiindo! Veja como:\")\n",
        "print(\"üìç PC1 (laranja): Captura a dire√ß√£o principal da varia√ß√£o (altura-peso correlacionados)\")\n",
        "print(\"üìç PC2 (azul): Captura a varia√ß√£o secund√°ria (perpendicular ao PC1)\")\n",
        "print(\"\\nüéØ Dica do Pedro: O PC1 √© onde os dados 'se espalham' mais!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé™ PCA: A Estrela Principal dos Autovetores\n\nAgora vamos para o **prato principal**: a **An√°lise de Componentes Principais (PCA)**!\n\nO PCA √© como um **organizador profissional** que entra na sua casa (dados) e diz:\n- \"Olha, essa dire√ß√£o aqui √© a mais importante\" (PC1)\n- \"Essa segunda dire√ß√£o tamb√©m importa, mas menos\" (PC2)\n- \"E essas outras... podemos descartar!\" (PCs menores)\n\n### üßÆ Matem√°tica do PCA\n\n**Passo 1**: Centralizar os dados\n$$\\mathbf{X}_{centrado} = \\mathbf{X} - \\bar{\\mathbf{X}}$$\n\n**Passo 2**: Calcular matriz de covari√¢ncia\n$$\\mathbf{C} = \\frac{1}{n-1}\\mathbf{X}_{centrado}^T\\mathbf{X}_{centrado}$$\n\n**Passo 3**: Encontrar autovetores e autovalores de $\\mathbf{C}$\n$$\\mathbf{C}\\mathbf{v}_i = \\lambda_i\\mathbf{v}_i$$\n\n**Passo 4**: Ordenar por autovalores (maior ‚Üí menor)\n\n**Passo 5**: Projetar dados nos componentes principais\n$$\\mathbf{Y} = \\mathbf{X}_{centrado}\\mathbf{V}$$\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/algebra-linear-para-ia-modulo-10_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PCA na ra√ßa (implementa√ß√£o manual)! üí™\n",
        "\n",
        "def pca_do_pedro(dados, n_componentes=2):\n",
        "    \"\"\"\n",
        "    PCA implementado na unha pelo Pedro! üî®\n",
        "    \"\"\"\n",
        "    print(\"üöÄ Executando PCA do Pedro...\")\n",
        "    \n",
        "    # Passo 1: Centralizar\n",
        "    dados_centrados = dados - np.mean(dados, axis=0)\n",
        "    print(\"‚úÖ Passo 1: Dados centralizados\")\n",
        "    \n",
        "    # Passo 2: Matriz de covari√¢ncia\n",
        "    cov_matrix = np.cov(dados_centrados.T)\n",
        "    print(\"‚úÖ Passo 2: Matriz de covari√¢ncia calculada\")\n",
        "    \n",
        "    # Passo 3: Autovetores e autovalores\n",
        "    autovalores, autovetores = np.linalg.eig(cov_matrix)\n",
        "    print(\"‚úÖ Passo 3: Autovetores e autovalores encontrados\")\n",
        "    \n",
        "    # Passo 4: Ordenar por import√¢ncia\n",
        "    idx_ordenado = np.argsort(autovalores)[::-1]\n",
        "    autovalores_ordenados = autovalores[idx_ordenado]\n",
        "    autovetores_ordenados = autovetores[:, idx_ordenado]\n",
        "    print(\"‚úÖ Passo 4: Ordena√ß√£o por import√¢ncia\")\n",
        "    \n",
        "    # Passo 5: Selecionar componentes principais\n",
        "    componentes_principais = autovetores_ordenados[:, :n_componentes]\n",
        "    \n",
        "    # Passo 6: Projetar dados\n",
        "    dados_transformados = dados_centrados @ componentes_principais\n",
        "    print(\"‚úÖ Passo 5: Proje√ß√£o conclu√≠da\")\n",
        "    \n",
        "    # Calculando vari√¢ncia explicada\n",
        "    variancia_explicada = autovalores_ordenados / np.sum(autovalores_ordenados)\n",
        "    \n",
        "    return {\n",
        "        'dados_transformados': dados_transformados,\n",
        "        'componentes_principais': componentes_principais,\n",
        "        'autovalores': autovalores_ordenados,\n",
        "        'variancia_explicada': variancia_explicada,\n",
        "        'dados_centrados': dados_centrados\n",
        "    }\n",
        "\n",
        "# Aplicando nosso PCA!\n",
        "resultado_pca = pca_do_pedro(dados, n_componentes=2)\n",
        "\n",
        "print(\"\\nüéâ Resultados do PCA:\")\n",
        "print(f\"Vari√¢ncia explicada por PC1: {resultado_pca['variancia_explicada'][0]*100:.1f}%\")\n",
        "print(f\"Vari√¢ncia explicada por PC2: {resultado_pca['variancia_explicada'][1]*100:.1f}%\")\n",
        "print(f\"Vari√¢ncia total explicada: {np.sum(resultado_pca['variancia_explicada'][:2])*100:.1f}%\")\n",
        "\n",
        "print(\"\\nüéØ Componentes Principais:\")\n",
        "for i, cp in enumerate(resultado_pca['componentes_principais'].T):\n",
        "    print(f\"PC{i+1}: [{cp[0]:.3f}, {cp[1]:.3f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparando com o PCA do scikit-learn! ‚öñÔ∏è\n",
        "\n",
        "# PCA do scikit-learn\n",
        "pca_sklearn = PCA(n_components=2)\n",
        "dados_pca_sklearn = pca_sklearn.fit_transform(dados)\n",
        "\n",
        "print(\"üî¨ Compara√ß√£o: PCA do Pedro vs Scikit-learn\")\n",
        "print(\"\\nVari√¢ncia explicada:\")\n",
        "print(f\"Pedro:    PC1={resultado_pca['variancia_explicada'][0]*100:.1f}%, PC2={resultado_pca['variancia_explicada'][1]*100:.1f}%\")\n",
        "print(f\"Sklearn:  PC1={pca_sklearn.explained_variance_ratio_[0]*100:.1f}%, PC2={pca_sklearn.explained_variance_ratio_[1]*100:.1f}%\")\n",
        "\n",
        "print(\"\\nComponentes principais:\")\n",
        "print(\"Pedro:\")\n",
        "for i, cp in enumerate(resultado_pca['componentes_principais'].T):\n",
        "    print(f\"  PC{i+1}: [{cp[0]:.3f}, {cp[1]:.3f}]\")\n",
        "\n",
        "print(\"Sklearn:\")\n",
        "for i, cp in enumerate(pca_sklearn.components_):\n",
        "    print(f\"  PC{i+1}: [{cp[0]:.3f}, {cp[1]:.3f}]\")\n",
        "\n",
        "# Visualiza√ß√£o comparativa\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# PCA do Pedro\n",
        "ax1.scatter(resultado_pca['dados_transformados'][:, 0], \n",
        "           resultado_pca['dados_transformados'][:, 1], \n",
        "           alpha=0.6, color='skyblue')\n",
        "ax1.set_title('üî® PCA do Pedro', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('PC1')\n",
        "ax1.set_ylabel('PC2')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
        "ax1.axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "# PCA do Scikit-learn\n",
        "ax2.scatter(dados_pca_sklearn[:, 0], dados_pca_sklearn[:, 1], \n",
        "           alpha=0.6, color='lightcoral')\n",
        "ax2.set_title('üî¨ PCA do Scikit-learn', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('PC1')\n",
        "ax2.set_ylabel('PC2')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
        "ax2.axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéâ Resultados praticamente id√™nticos! Nosso PCA t√° funcionando perfeitamente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üå∫ Exemplo Real: Dataset Iris\n\nBora aplicar PCA num dataset cl√°ssico: o **Iris**! üå∏\n\nO Iris tem 4 dimens√µes (features):\n1. Comprimento da s√©pala\n2. Largura da s√©pala  \n3. Comprimento da p√©tala\n4. Largura da p√©tala\n\nVamos usar PCA para reduzir de **4D para 2D** e ainda assim manter a informa√ß√£o essencial!\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/algebra-linear-para-ia-modulo-10_img_03.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregando o famoso dataset Iris! üå∫\n",
        "\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "nomes_features = iris.feature_names\n",
        "nomes_especies = iris.target_names\n",
        "\n",
        "print(\"üå∫ Dataset Iris carregado!\")\n",
        "print(f\"Shape: {X_iris.shape}\")\n",
        "print(f\"Features: {nomes_features}\")\n",
        "print(f\"Esp√©cies: {nomes_especies}\")\n",
        "\n",
        "# Estat√≠sticas b√°sicas\n",
        "df_iris = pd.DataFrame(X_iris, columns=nomes_features)\n",
        "df_iris['especie'] = y_iris\n",
        "\n",
        "print(\"\\nüìä Estat√≠sticas por esp√©cie:\")\n",
        "for i, especie in enumerate(nomes_especies):\n",
        "    dados_especie = df_iris[df_iris['especie'] == i]\n",
        "    print(f\"\\n{especie.capitalize()}:\")\n",
        "    print(dados_especie.describe().round(2))\n",
        "\n",
        "# Padronizando os dados (importante para PCA!)\n",
        "scaler = StandardScaler()\n",
        "X_iris_scaled = scaler.fit_transform(X_iris)\n",
        "\n",
        "print(\"\\n‚úÖ Dados padronizados (m√©dia=0, std=1)\")\n",
        "print(f\"M√©dias ap√≥s padroniza√ß√£o: {np.mean(X_iris_scaled, axis=0).round(3)}\")\n",
        "print(f\"Desvios ap√≥s padroniza√ß√£o: {np.std(X_iris_scaled, axis=0).round(3)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplicando PCA no Iris! üéØ\n",
        "\n",
        "# PCA para 2 componentes\n",
        "pca_iris = PCA(n_components=4)  # Primeiro vamos ver todos os componentes\n",
        "X_iris_pca = pca_iris.fit_transform(X_iris_scaled)\n",
        "\n",
        "print(\"üî• An√°lise de Componentes Principais - Iris\")\n",
        "print(\"\\nVari√¢ncia explicada por cada componente:\")\n",
        "for i, var_exp in enumerate(pca_iris.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {var_exp*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nVari√¢ncia acumulada:\")\n",
        "var_acumulada = np.cumsum(pca_iris.explained_variance_ratio_)\n",
        "for i, var_acc in enumerate(var_acumulada):\n",
        "    print(f\"PC1 at√© PC{i+1}: {var_acc*100:.1f}%\")\n",
        "\n",
        "print(\"\\nüéØ Componentes Principais (autovetores):\")\n",
        "componentes_df = pd.DataFrame(\n",
        "    pca_iris.components_.T,\n",
        "    columns=[f'PC{i+1}' for i in range(4)],\n",
        "    index=nomes_features\n",
        ")\n",
        "print(componentes_df.round(3))\n",
        "\n",
        "# Interpreta√ß√£o dos componentes\n",
        "print(\"\\nüß† Interpreta√ß√£o dos Componentes:\")\n",
        "print(\"PC1: Captura o tamanho geral da flor (todas as features com pesos similares)\")\n",
        "print(\"PC2: Contraste entre s√©pala e p√©tala (s√©pala positiva, p√©tala negativa)\")\n",
        "print(\"PC3 e PC4: Varia√ß√µes mais sutis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√£o √©pica do PCA no Iris! üé®\n",
        "\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "cores = ['red', 'green', 'blue']\n",
        "markers = ['o', 's', '^']\n",
        "\n",
        "# Plot 1: Vari√¢ncia explicada\n",
        "ax1.bar(range(1, 5), pca_iris.explained_variance_ratio_*100, \n",
        "        color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'], alpha=0.8)\n",
        "ax1.set_xlabel('Componente Principal')\n",
        "ax1.set_ylabel('Vari√¢ncia Explicada (%)')\n",
        "ax1.set_title('üìä Vari√¢ncia Explicada por Componente', fontweight='bold')\n",
        "ax1.set_xticks(range(1, 5))\n",
        "for i, v in enumerate(pca_iris.explained_variance_ratio_*100):\n",
        "    ax1.text(i+1, v+1, f'{v:.1f}%', ha='center', fontweight='bold')\n",
        "\n",
        "# Plot 2: Vari√¢ncia acumulada\n",
        "ax2.plot(range(1, 5), var_acumulada*100, 'o-', linewidth=3, markersize=8, color='#FF6B6B')\n",
        "ax2.fill_between(range(1, 5), var_acumulada*100, alpha=0.3, color='#FF6B6B')\n",
        "ax2.axhline(y=95, color='green', linestyle='--', alpha=0.7, label='95% threshold')\n",
        "ax2.set_xlabel('N√∫mero de Componentes')\n",
        "ax2.set_ylabel('Vari√¢ncia Acumulada (%)')\n",
        "ax2.set_title('üìà Vari√¢ncia Acumulada', fontweight='bold')\n",
        "ax2.set_xticks(range(1, 5))\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: PC1 vs PC2\n",
        "for i, especie in enumerate(nomes_especies):\n",
        "    mask = y_iris == i\n",
        "    ax3.scatter(X_iris_pca[mask, 0], X_iris_pca[mask, 1], \n",
        "               c=cores[i], marker=markers[i], s=80, alpha=0.7, \n",
        "               label=especie.capitalize())\n",
        "\n",
        "ax3.set_xlabel(f'PC1 ({pca_iris.explained_variance_ratio_[0]*100:.1f}%)')\n",
        "ax3.set_ylabel(f'PC2 ({pca_iris.explained_variance_ratio_[1]*100:.1f}%)')\n",
        "ax3.set_title('üå∫ Iris no Espa√ßo dos Componentes Principais', fontweight='bold')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Heatmap dos componentes\n",
        "im = ax4.imshow(pca_iris.components_, cmap='RdBu_r', aspect='auto')\n",
        "ax4.set_xticks(range(len(nomes_features)))\n",
        "ax4.set_xticklabels([nome.replace(' (cm)', '').replace('sepal', 'sep').replace('petal', 'pet') \n",
        "                    for nome in nomes_features], rotation=45)\n",
        "ax4.set_yticks(range(4))\n",
        "ax4.set_yticklabels([f'PC{i+1}' for i in range(4)])\n",
        "ax4.set_title('üéØ Pesos dos Componentes Principais', fontweight='bold')\n",
        "\n",
        "# Adicionando valores no heatmap\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        text = ax4.text(j, i, f'{pca_iris.components_[i, j]:.2f}',\n",
        "                       ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
        "\n",
        "plt.colorbar(im, ax=ax4, shrink=0.8)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéâ Liiindo! Veja como PC1 e PC2 capturam 95.8% da vari√¢ncia!\")\n",
        "print(\"üìç As tr√™s esp√©cies ficaram bem separadas no espa√ßo 2D!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Diagrama do Processo PCA\n\nVamos visualizar todo o processo do PCA de forma clara:\n\n```mermaid\ngraph LR\n    A[Dados Originais<br/>4D: Iris] --> B[Padroniza√ß√£o<br/>Œº=0, œÉ=1]\n    B --> C[Matriz Covari√¢ncia<br/>4x4]\n    C --> D[Autovetores<br/>Autovalores]\n    D --> E[Ordena√ß√£o<br/>por Import√¢ncia]\n    E --> F[Sele√ß√£o PC1, PC2<br/>95.8% vari√¢ncia]\n    F --> G[Proje√ß√£o<br/>4D ‚Üí 2D]\n    G --> H[Visualiza√ß√£o<br/>Classes Separadas]\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèãÔ∏è‚Äç‚ôÇÔ∏è Exerc√≠cio 1: PCA em Dados Sint√©ticos\n\nAgora √© sua vez! Vamos criar um desafio pr√°tico para voc√™ fixar o conhecimento.\n\n**üéØ Seu desafio:**\n1. Crie dados sint√©ticos 3D com correla√ß√µes espec√≠ficas\n2. Aplique PCA e analise os resultados\n3. Visualize antes e depois da transforma√ß√£o\n\n**üéØ Dica do Pedro**: Lembre-se de centralizar os dados antes de calcular a covari√¢ncia!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üèãÔ∏è‚Äç‚ôÇÔ∏è EXERC√çCIO 1: Complete o c√≥digo abaixo!\n",
        "\n",
        "np.random.seed(123)\n",
        "n_samples = 300\n",
        "\n",
        "# TODO: Crie dados 3D correlacionados\n",
        "# Dica: Use np.random.multivariate_normal com uma matriz de covari√¢ncia interessante\n",
        "\n",
        "# Matriz de covari√¢ncia (voc√™ pode modificar!)\n",
        "cov_matrix_ex = np.array([[4, 2, 1],\n",
        "                         [2, 3, 0.5],\n",
        "                         [1, 0.5, 1]])\n",
        "\n",
        "# TODO: Gere os dados usando a matriz de covari√¢ncia\n",
        "dados_3d = # SEU C√ìDIGO AQUI\n",
        "\n",
        "print(f\"üìä Dados 3D criados: {dados_3d.shape}\")\n",
        "\n",
        "# TODO: Aplique PCA usando nossa fun√ß√£o pca_do_pedro\n",
        "resultado_3d = # SEU C√ìDIGO AQUI\n",
        "\n",
        "# TODO: Calcule quantos componentes explicam 90% da vari√¢ncia\n",
        "var_acum = # SEU C√ìDIGO AQUI\n",
        "n_comp_90 = # SEU C√ìDIGO AQUI\n",
        "\n",
        "print(f\"\\nüéØ Componentes necess√°rios para 90% da vari√¢ncia: {n_comp_90}\")\n",
        "\n",
        "# TODO: Crie uma visualiza√ß√£o 3D dos dados originais e 2D dos transformados\n",
        "# Use matplotlib para plotar side-by-side\n",
        "\n",
        "# ESPA√áO PARA SEU C√ìDIGO DE VISUALIZA√á√ÉO\n",
        "\n",
        "print(\"\\nüéâ Exerc√≠cio conclu√≠do! Como ficaram seus resultados?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéÆ Exerc√≠cio 2: PCA para Compress√£o de Imagens\n\nAgora vamos aplicar PCA para **compress√£o de imagens**! üì∏\n\nVamos criar uma imagem sint√©tica e usar PCA para comprimi-la, mantendo apenas os componentes mais importantes.\n\n**üéØ Conceito**: Cada linha da imagem √© tratada como um ponto no espa√ßo de alta dimensionalidade. O PCA encontra as \"dire√ß√µes principais\" da varia√ß√£o entre as linhas.\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/algebra-linear-para-ia-modulo-10_img_04.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéÆ EXERC√çCIO 2: Compress√£o de Imagem com PCA\n",
        "\n",
        "# Criando uma imagem sint√©tica interessante\n",
        "def criar_imagem_sintetica(altura=100, largura=100):\n",
        "    \"\"\"\n",
        "    Cria uma imagem sint√©tica com padr√µes interessantes\n",
        "    \"\"\"\n",
        "    x = np.linspace(-2, 2, largura)\n",
        "    y = np.linspace(-2, 2, altura)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    \n",
        "    # Padr√£o interessante: ondas + gradiente\n",
        "    imagem = np.sin(3*X) * np.cos(3*Y) + 0.5 * (X + Y)\n",
        "    \n",
        "    # Normalizando para [0, 1]\n",
        "    imagem = (imagem - imagem.min()) / (imagem.max() - imagem.min())\n",
        "    \n",
        "    return imagem\n",
        "\n",
        "# TODO: Crie a imagem sint√©tica\n",
        "imagem_original = criar_imagem_sintetica(80, 80)\n",
        "\n",
        "print(f\"üñºÔ∏è Imagem criada: {imagem_original.shape}\")\n",
        "\n",
        "# TODO: Aplique PCA para diferentes n√∫meros de componentes\n",
        "# Dica: Trate cada linha da imagem como um ponto de dados\n",
        "\n",
        "componentes_testados = [5, 10, 20, 40]\n",
        "imagens_comprimidas = {}\n",
        "\n",
        "for n_comp in componentes_testados:\n",
        "    # TODO: Aplique PCA com n_comp componentes\n",
        "    pca_img = PCA(n_components=n_comp)\n",
        "    \n",
        "    # Transforme e depois reconstrua a imagem\n",
        "    # DICA: fit_transform() e depois inverse_transform()\n",
        "    \n",
        "    # SEU C√ìDIGO AQUI\n",
        "    dados_transformados = # COMPLETE\n",
        "    imagem_reconstruida = # COMPLETE\n",
        "    \n",
        "    imagens_comprimidas[n_comp] = {\n",
        "        'imagem': imagem_reconstruida,\n",
        "        'variancia_explicada': pca_img.explained_variance_ratio_.sum(),\n",
        "        'taxa_compressao': n_comp / imagem_original.shape[1]\n",
        "    }\n",
        "\n",
        "# TODO: Visualize os resultados\n",
        "# Crie um subplot mostrando a imagem original e as compress√µes\n",
        "\n",
        "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
        "\n",
        "# Imagem original\n",
        "axes[0].imshow(imagem_original, cmap='viridis')\n",
        "axes[0].set_title('üñºÔ∏è Original')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# TODO: Complete o loop de visualiza√ß√£o\n",
        "for i, n_comp in enumerate(componentes_testados):\n",
        "    # SEU C√ìDIGO DE VISUALIZA√á√ÉO AQUI\n",
        "    pass\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# TODO: Calcule e mostre as m√©tricas de compress√£o\n",
        "print(\"\\nüìä M√©tricas de Compress√£o:\")\n",
        "for n_comp in componentes_testados:\n",
        "    dados = imagens_comprimidas[n_comp]\n",
        "    print(f\"{n_comp:2d} componentes: {dados['variancia_explicada']*100:.1f}% vari√¢ncia, \"\n",
        "          f\"taxa compress√£o: {dados['taxa_compressao']*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîó Conex√µes com os M√≥dulos Anteriores\n\nAgora que dominamos autovetores e PCA, vamos conectar com tudo que aprendemos no curso!\n\n### üéØ **M√≥dulo 2 - Produto Escalar**\n- A **proje√ß√£o** nos componentes principais usa produto escalar!\n- $PC_i = \\mathbf{X} \\cdot \\mathbf{v}_i$ (onde $\\mathbf{v}_i$ √© o autovetor)\n\n### üéØ **M√≥dulo 3 - Multiplica√ß√£o de Matrizes**\n- Toda transforma√ß√£o PCA √© multiplica√ß√£o: $\\mathbf{Y} = \\mathbf{X}\\mathbf{V}$\n- Lembra da ordem das matrizes? Crucial aqui!\n\n### üéØ **M√≥dulo 6 - Transforma√ß√µes Lineares**\n- PCA **√©** uma transforma√ß√£o linear!\n- Mudamos de base: do espa√ßo original para o espa√ßo dos PCs\n\n### üéØ **M√≥dulo 7 - Transposta**\n- Matriz de covari√¢ncia: $\\mathbf{C} = \\mathbf{X}^T\\mathbf{X}$\n- Autovetores s√£o **ortonormais** (lembra das propriedades?)\n\n### üéØ **M√≥dulo 9 - SVD**\n- **PLOT TWIST**: PCA e SVD s√£o primos! ü§Ø\n- Os autovetores da covari√¢ncia s√£o os **vetores singulares direitos** da SVD!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conectando PCA e SVD - A revela√ß√£o final! ü§Ø\n",
        "\n",
        "# Usando dados do Iris padronizados\n",
        "X = X_iris_scaled\n",
        "\n",
        "print(\"üîó Conectando PCA e SVD - A Revela√ß√£o Final!\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# M√©todo 1: PCA cl√°ssico (autovetores da matriz de covari√¢ncia)\n",
        "pca_classico = PCA(n_components=2)\n",
        "X_pca = pca_classico.fit_transform(X)\n",
        "\n",
        "# M√©todo 2: SVD direto nos dados centralizados\n",
        "X_centrado = X - np.mean(X, axis=0)\n",
        "U, s, Vt = np.linalg.svd(X_centrado, full_matrices=False)\n",
        "\n",
        "# Os componentes principais s√£o as colunas de Vt.T!\n",
        "componentes_svd = Vt.T[:, :2]\n",
        "X_svd = X_centrado @ componentes_svd\n",
        "\n",
        "print(\"üéØ Componentes Principais - PCA:\")\n",
        "print(pca_classico.components_.T[:, :2].round(3))\n",
        "\n",
        "print(\"\\nüéØ Componentes Principais - SVD:\")\n",
        "print(componentes_svd.round(3))\n",
        "\n",
        "print(\"\\nü§ù S√£o iguais? (pode ter sinais opostos)\")\n",
        "for i in range(2):\n",
        "    pc_pca = pca_classico.components_[i]\n",
        "    pc_svd = Vt[i]\n",
        "    \n",
        "    # Testando se s√£o iguais ou opostos\n",
        "    igual = np.allclose(pc_pca, pc_svd) or np.allclose(pc_pca, -pc_svd)\n",
        "    print(f\"PC{i+1}: {igual}\")\n",
        "\n",
        "# Visualiza√ß√£o da equival√™ncia\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "cores = ['red', 'green', 'blue']\n",
        "for i, especie in enumerate(nomes_especies):\n",
        "    mask = y_iris == i\n",
        "    \n",
        "    ax1.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
        "               c=cores[i], alpha=0.7, label=especie.capitalize())\n",
        "    \n",
        "    ax2.scatter(X_svd[mask, 0], X_svd[mask, 1], \n",
        "               c=cores[i], alpha=0.7, label=especie.capitalize())\n",
        "\n",
        "ax1.set_title('üéØ PCA Cl√°ssico (Autovetores)', fontweight='bold')\n",
        "ax1.set_xlabel('PC1')\n",
        "ax1.set_ylabel('PC2')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.set_title('üîó SVD (Vetores Singulares)', fontweight='bold')\n",
        "ax2.set_xlabel('PC1')\n",
        "ax2.set_ylabel('PC2')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéâ LIIINDO! PCA e SVD s√£o dois caminhos para o mesmo destino!\")\n",
        "print(\"üîó Os autovetores da covari√¢ncia = vetores singulares direitos da SVD!\")\n",
        "print(\"\\nüéØ Dica do Pedro: Na pr√°tica, SVD √© mais est√°vel numericamente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Aplica√ß√µes Reais dos Autovetores na IA\n\nBora ver onde esses conceitos aparecem no mundo real da IA! üöÄ\n\n### üß† **1. Redes Neurais Convolucionais (CNNs)**\n- **Pooling layers** fazem uma forma de redu√ß√£o dimensional\n- **Feature maps** capturam as \"dire√ß√µes importantes\" da imagem\n\n### üîç **2. Sistemas de Recomenda√ß√£o**\n- **Matrix Factorization**: Netflix, Spotify, Amazon\n- Encontra \"gostos principais\" (autovetores) dos usu√°rios\n\n### üìä **3. An√°lise de Sentimentos**\n- **Embeddings** de palavras (Word2Vec, GloVe)\n- PCA reduz dimensionalidade para visualiza√ß√£o\n\n### üé≠ **4. Reconhecimento Facial**\n- **Eigenfaces**: rostos como combina√ß√£o de \"faces principais\"\n- Cada autovetor √© uma \"face base\"\n\n### üè• **5. An√°lise de Dados M√©dicos**\n- Reduz milhares de genes para padr√µes principais\n- Identifica **biomarcadores** importantes\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/algebra-linear-para-ia-modulo-10_img_05.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulando um Sistema de Recomenda√ß√£o com PCA! üé¨\n",
        "\n",
        "print(\"üé¨ Simulando Netflix com PCA!\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Criando dados sint√©ticos de avalia√ß√£o de filmes\n",
        "np.random.seed(42)\n",
        "\n",
        "# 100 usu√°rios, 50 filmes\n",
        "n_usuarios = 100\n",
        "n_filmes = 50\n",
        "\n",
        "# Tipos de usu√°rios (gostos latentes)\n",
        "tipos_usuarios = {\n",
        "    'acao': [1, 0, 0],      # Gosta de a√ß√£o\n",
        "    'romance': [0, 1, 0],   # Gosta de romance  \n",
        "    'comedia': [0, 0, 1],   # Gosta de com√©dia\n",
        "    'misto': [0.5, 0.5, 0.5] # Gosta de tudo\n",
        "}\n",
        "\n",
        "# Tipos de filmes\n",
        "tipos_filmes = np.random.choice(['acao', 'romance', 'comedia'], n_filmes)\n",
        "filmes_features = np.zeros((n_filmes, 3))\n",
        "\n",
        "for i, tipo in enumerate(tipos_filmes):\n",
        "    if tipo == 'acao':\n",
        "        filmes_features[i] = [1, 0, 0]\n",
        "    elif tipo == 'romance':\n",
        "        filmes_features[i] = [0, 1, 0]\n",
        "    else:  # com√©dia\n",
        "        filmes_features[i] = [0, 0, 1]\n",
        "\n",
        "# Gerando prefer√™ncias dos usu√°rios\n",
        "usuarios_preferencias = np.zeros((n_usuarios, 3))\n",
        "for i in range(n_usuarios):\n",
        "    tipo_usuario = np.random.choice(list(tipos_usuarios.keys()))\n",
        "    usuarios_preferencias[i] = tipos_usuarios[tipo_usuario]\n",
        "    # Adicionando um pouco de ru√≠do\n",
        "    usuarios_preferencias[i] += np.random.normal(0, 0.1, 3)\n",
        "\n",
        "# Matriz de avalia√ß√µes (usu√°rios x filmes)\n",
        "avaliacoes = usuarios_preferencias @ filmes_features.T\n",
        "# Convertendo para escala 1-5 e adicionando ru√≠do\n",
        "avaliacoes = 3 + 2 * avaliacoes + np.random.normal(0, 0.3, avaliacoes.shape)\n",
        "avaliacoes = np.clip(avaliacoes, 1, 5)\n",
        "\n",
        "print(f\"üìä Matriz de avalia√ß√µes: {avaliacoes.shape}\")\n",
        "print(f\"M√©dia das avalia√ß√µes: {np.mean(avaliacoes):.2f}\")\n",
        "print(f\"Filmes por g√™nero: {dict(zip(*np.unique(tipos_filmes, return_counts=True)))}\")\n",
        "\n",
        "# Aplicando PCA para encontrar \"gostos principais\"\n",
        "pca_recomendacao = PCA(n_components=3)\n",
        "usuarios_pca = pca_recomendacao.fit_transform(avaliacoes)\n",
        "\n",
        "print(\"\\nüéØ Componentes Principais (Gostos Latentes):\")\n",
        "for i, var_exp in enumerate(pca_recomendacao.explained_variance_ratio_):\n",
        "    print(f\"Gosto {i+1}: {var_exp*100:.1f}% da varia√ß√£o\")\n",
        "\n",
        "# Visualiza√ß√£o dos usu√°rios no espa√ßo dos gostos\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Plot 1: PC1 vs PC2\n",
        "ax1 = fig.add_subplot(131)\n",
        "scatter = ax1.scatter(usuarios_pca[:, 0], usuarios_pca[:, 1], \n",
        "                     c=usuarios_preferencias[:, 0], cmap='viridis', alpha=0.7)\n",
        "ax1.set_xlabel('Gosto Principal 1')\n",
        "ax1.set_ylabel('Gosto Principal 2')\n",
        "ax1.set_title('üé≠ Usu√°rios por Gostos Latentes')\n",
        "plt.colorbar(scatter, ax=ax1, label='Prefer√™ncia A√ß√£o')\n",
        "\n",
        "# Plot 2: Vari√¢ncia explicada\n",
        "ax2 = fig.add_subplot(132)\n",
        "ax2.bar(range(1, 4), pca_recomendacao.explained_variance_ratio_*100, \n",
        "        color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
        "ax2.set_xlabel('Componente')\n",
        "ax2.set_ylabel('Vari√¢ncia Explicada (%)')\n",
        "ax2.set_title('üìä Import√¢ncia dos Gostos')\n",
        "\n",
        "# Plot 3: Heatmap dos componentes\n",
        "ax3 = fig.add_subplot(133)\n",
        "# Calculando quais filmes mais representam cada componente\n",
        "filmes_transformados = pca_recomendacao.transform(avaliacoes.T)\n",
        "im = ax3.imshow(pca_recomendacao.components_[:, :20], cmap='RdBu_r', aspect='auto')\n",
        "ax3.set_xlabel('Primeiros 20 Filmes')\n",
        "ax3.set_ylabel('Componentes')\n",
        "ax3.set_title('üé¨ Filmes vs Gostos')\n",
        "plt.colorbar(im, ax=ax3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéâ Sistema de recomenda√ß√£o funcionando!\")\n",
        "print(\"üìç Cada usu√°rio agora tem coordenadas nos 'gostos principais'\")\n",
        "print(\"üìç Usu√°rios pr√≥ximos no espa√ßo PCA t√™m gostos similares!\")\n",
        "print(\"\\nüéØ Dica do Pedro: Netflix usa t√©cnicas similares, mas muito mais sofisticadas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéä Resumo Final: A Jornada dos Autovetores\n\nParab√©ns! üéâ Voc√™ completou a jornada pelos **autovetores e autovalores**!\n\n### üß† **O que aprendemos:**\n\n1. **üéØ Conceito Fundamental**\n   - Autovetores: dire√ß√µes que **n√£o mudam** na transforma√ß√£o\n   - Autovalores: **quanto** a dire√ß√£o √© esticada/comprimida\n   - Equa√ß√£o: $A\\mathbf{v} = \\lambda\\mathbf{v}$\n\n2. **üìä PCA (An√°lise de Componentes Principais)**\n   - Encontra as **dire√ß√µes de maior varia√ß√£o** nos dados\n   - Reduz dimensionalidade mantendo informa√ß√£o essencial\n   - Baseado nos autovetores da matriz de covari√¢ncia\n\n3. **üîó Conex√µes Profundas**\n   - PCA = autovetores da covari√¢ncia\n   - SVD = outra forma de chegar no mesmo resultado\n   - Transforma√ß√µes lineares em a√ß√£o!\n\n4. **üöÄ Aplica√ß√µes Pr√°ticas**\n   - Compress√£o de dados/imagens\n   - Sistemas de recomenda√ß√£o\n   - An√°lise explorat√≥ria de dados\n   - Preprocessamento para ML\n\n### üéØ **Dicas Finais do Pedro:**\n\n‚úÖ **Sempre centralize** os dados antes do PCA  \n‚úÖ **Padronize** quando as features t√™m escalas diferentes  \n‚úÖ **Analise a vari√¢ncia explicada** para escolher quantos componentes manter  \n‚úÖ **Interprete os componentes** - eles t√™m significado real!  \n‚úÖ **SVD √© mais est√°vel** numericamente que autovetores  \n\n### üèÜ **Voc√™ agora domina:**\n- ‚úÖ **M√≥dulo 1**: Vetores e Matrizes  \n- ‚úÖ **M√≥dulo 2**: Produto Escalar  \n- ‚úÖ **M√≥dulo 3**: Multiplica√ß√£o de Matrizes  \n- ‚úÖ **M√≥dulo 4**: NumPy  \n- ‚úÖ **M√≥dulo 5**: Sistemas Lineares  \n- ‚úÖ **M√≥dulo 6**: Transforma√ß√µes Lineares  \n- ‚úÖ **M√≥dulo 7**: Inversa e Transposta  \n- ‚úÖ **M√≥dulo 8**: Determinante  \n- ‚úÖ **M√≥dulo 9**: SVD  \n- ‚úÖ **M√≥dulo 10**: **Autovetores e PCA** üéØ\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/algebra-linear-para-ia-modulo-10_img_06.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéä CELEBRA√á√ÉO FINAL! üéä\n",
        "\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import FancyBboxPatch\n",
        "\n",
        "# Criando um certificado visual! üèÜ\n",
        "fig, ax = plt.subplots(figsize=(16, 10))\n",
        "\n",
        "# Fundo do certificado\n",
        "fancy_box = FancyBboxPatch((0.05, 0.05), 0.9, 0.9, \n",
        "                          boxstyle=\"round,pad=0.02\", \n",
        "                          facecolor='lightblue', \n",
        "                          edgecolor='navy', \n",
        "                          linewidth=3)\n",
        "ax.add_patch(fancy_box)\n",
        "\n",
        "# T√≠tulo principal\n",
        "ax.text(0.5, 0.85, 'üèÜ CERTIFICADO DE CONCLUS√ÉO üèÜ', \n",
        "        fontsize=24, fontweight='bold', ha='center', va='center',\n",
        "        color='navy')\n",
        "\n",
        "ax.text(0.5, 0.75, '√ÅLGEBRA LINEAR PARA IA', \n",
        "        fontsize=20, fontweight='bold', ha='center', va='center',\n",
        "        color='darkblue')\n",
        "\n",
        "# Conte√∫do\n",
        "ax.text(0.5, 0.65, 'Voc√™ dominou com sucesso:', \n",
        "        fontsize=16, ha='center', va='center', color='black')\n",
        "\n",
        "modulos = [\n",
        "    '‚úÖ Autovetores e Autovalores',\n",
        "    '‚úÖ An√°lise de Componentes Principais (PCA)', \n",
        "    '‚úÖ Redu√ß√£o de Dimensionalidade',\n",
        "    '‚úÖ Aplica√ß√µes em IA e ML',\n",
        "    '‚úÖ Conex√µes com SVD e Transforma√ß√µes'\n",
        "]\n",
        "\n",
        "y_pos = 0.55\n",
        "for modulo in modulos:\n",
        "    ax.text(0.5, y_pos, modulo, fontsize=14, ha='center', va='center', \n",
        "            color='darkgreen', fontweight='bold')\n",
        "    y_pos -= 0.06\n",
        "\n",
        "# Assinatura do Pedro\n",
        "ax.text(0.5, 0.15, 'Pedro Nunes Guth', \n",
        "        fontsize=18, ha='center', va='center', \n",
        "        color='navy', fontweight='bold', style='italic')\n",
        "\n",
        "ax.text(0.5, 0.08, 'Expert em IA, Matem√°tica e AWS', \n",
        "        fontsize=12, ha='center', va='center', color='gray')\n",
        "\n",
        "# Removendo eixos\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéâ\" * 50)\n",
        "print(\"üéä PARAB√âNS! CURSO CONCLU√çDO COM SUCESSO! üéä\")\n",
        "print(\"üéâ\" * 50)\n",
        "print()\n",
        "print(\"üöÄ Voc√™ agora tem uma base s√≥lida em √Ålgebra Linear para IA!\")\n",
        "print(\"üß† Entende como funcionam PCA, SVD, transforma√ß√µes lineares...\")\n",
        "print(\"üí™ Pode aplicar esses conceitos em projetos reais de IA!\")\n",
        "print()\n",
        "print(\"üéØ Pr√≥ximos passos sugeridos:\")\n",
        "print(\"   üìö Deep Learning (CNNs, RNNs, Transformers)\")\n",
        "print(\"   üìä Machine Learning avan√ßado\")\n",
        "print(\"   üî¨ Estat√≠stica e Probabilidade\")\n",
        "print(\"   ‚òÅÔ∏è  MLOps e deploy na AWS\")\n",
        "print()\n",
        "print(\"Obrigado por essa jornada incr√≠vel! üôè\")\n",
        "print(\"Lembre-se: a matem√°tica √© a linguagem da IA! ü§ñ\")\n",
        "print()\n",
        "print(\"#AlgebraLinear #IA #MachineLearning #PCA #Autovetores\")\n",
        "print(\"üéâ\" * 50)"
      ]
    }
  ]
}