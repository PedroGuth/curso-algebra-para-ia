{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîç SVD: Decomposi√ß√£o que Revela o Essencial dos Dados\n\n## M√≥dulo 9: √Ålgebra Linear para IA\n\nFala pessoal! üëã Chegamos no m√≥dulo 9 e agora vamos falar de uma das t√©cnicas mais poderosas da √°lgebra linear: a **Decomposi√ß√£o SVD** (Singular Value Decomposition)!\n\nT√°, mas o que √© essa tal de SVD? Imagina que voc√™ tem uma foto de 4K ultra detalhada, mas precisa envi√°-la pelo WhatsApp sem perder muito da qualidade. Ou ent√£o voc√™ tem um sistema de recomenda√ß√£o do Netflix que precisa entender os padr√µes de milh√µes de usu√°rios. A SVD √© como um \"detector de padr√µes essenciais\" que consegue pegar o que realmente importa nos seus dados!\n\n**Neste notebook voc√™ vai aprender:**\n- üßÆ O que √© SVD matematicamente (de forma descomplicada!)\n- üì∏ Como comprimir imagens mantendo a qualidade\n- üé¨ Como criar sistemas de recomenda√ß√£o\n- üìä Redu√ß√£o de dimensionalidade na pr√°tica\n- üîß Implementa√ß√£o em Python com NumPy\n\nBora descobrir o essencial dos dados! üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inicial - importando as bibliotecas que vamos usar\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_blobs\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Configura√ß√µes para gr√°ficos mais bonitos\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Seed para reprodutibilidade\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"üîß Setup completo! Vamos come√ßar a descobrir o essencial dos dados!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ü§î O Que √â SVD? Quebrando o Conceito\n\nLembra quando falamos sobre **transforma√ß√µes lineares** no M√≥dulo 6? A SVD √© como se fosse uma \"receita de bolo\" para decompor qualquer matriz em tr√™s ingredientes fundamentais!\n\nMatematicamente, para qualquer matriz $A$ de dimens√µes $m \\times n$, a SVD nos d√°:\n\n$$A = U \\Sigma V^T$$\n\nOnde:\n- $U$ √© uma matriz $m \\times m$ ortogonal (colunas s√£o ortonormais)\n- $\\Sigma$ √© uma matriz diagonal $m \\times n$ com valores singulares\n- $V^T$ √© uma matriz $n \\times n$ ortogonal transposta\n\n### üè† Analogia da Casa\n\nPensa na sua matriz $A$ como uma casa:\n- $U$: As **dire√ß√µes dos c√¥modos** (como os dados se espalham no espa√ßo linha)\n- $\\Sigma$: A **import√¢ncia de cada c√¥modo** (valores singulares em ordem decrescente)\n- $V^T$: As **dire√ß√µes dos m√≥veis** (como os dados se espalham no espa√ßo coluna)\n\nA magia acontece porque os valores em $\\Sigma$ est√£o ordenados do maior para o menor. Os maiores valores capturam os padr√µes mais importantes!\n\n**üéØ Dica do Pedro:** A SVD sempre existe para qualquer matriz! Diferente da decomposi√ß√£o em autovalores que s√≥ funciona para matrizes quadradas, a SVD √© universal!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar uma matriz simples para ver a SVD em a√ß√£o\n",
        "# Essa matriz representa dados com padr√µes claros\n",
        "A = np.array([\n",
        "    [1, 2, 3],\n",
        "    [2, 4, 6],\n",
        "    [1, 3, 5],\n",
        "    [3, 6, 9]\n",
        "])\n",
        "\n",
        "print(\"üè† Nossa matriz A (a 'casa' que vamos decompor):\")\n",
        "print(A)\n",
        "print(f\"\\nDimens√µes: {A.shape[0]} linhas x {A.shape[1]} colunas\")\n",
        "\n",
        "# Aplicando SVD\n",
        "U, sigma, Vt = np.linalg.svd(A)\n",
        "\n",
        "print(\"\\nüîç Componentes da SVD:\")\n",
        "print(f\"U (dire√ß√µes dos c√¥modos): {U.shape}\")\n",
        "print(f\"Sigma (import√¢ncia): {sigma.shape}\")\n",
        "print(f\"V^T (dire√ß√µes dos m√≥veis): {Vt.shape}\")\n",
        "\n",
        "print(\"\\nüìä Valores singulares (em ordem decrescente):\")\n",
        "print(sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. üßÆ A Matem√°tica por Tr√°s da SVD\n\nT√°, mas como diabos a SVD funciona matematicamente? Vou te explicar de forma descomplicada!\n\n### üîç Os Valores Singulares\n\nOs valores singulares $\\sigma_i$ s√£o as ra√≠zes quadradas dos autovalores de $A^T A$ (ou $A A^T$):\n\n$$\\sigma_i = \\sqrt{\\lambda_i(A^T A)}$$\n\n### üéØ As Matrizes U e V\n\n- $U$: Cont√©m os **vetores singulares √† esquerda** (autovetores de $AA^T$)\n- $V$: Cont√©m os **vetores singulares √† direita** (autovetores de $A^T A$)\n\n### üèóÔ∏è Propriedades Importantes\n\n1. **Ortogonalidade**: $U^T U = I$ e $V^T V = I$\n2. **Ordena√ß√£o**: $\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_r \\geq 0$\n3. **Posto**: O n√∫mero de valores singulares n√£o-nulos = posto da matriz\n\n### üß© Reconstru√ß√£o da Matriz\n\nA matriz original pode ser reconstru√≠da como:\n\n$$A = \\sum_{i=1}^{r} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T$$\n\nOnde $r$ √© o posto da matriz. Cada termo $\\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T$ √© uma matriz de posto 1!\n\n**üéØ Dica do Pedro:** Pensa na SVD como uma \"receita\" que te diz exatamente como misturar padr√µes simples (matrizes de posto 1) para reconstruir seus dados complexos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos verificar as propriedades matem√°ticas da SVD\n",
        "print(\"üî¨ Verificando as propriedades matem√°ticas da SVD:\\n\")\n",
        "\n",
        "# 1. Verificar se U^T * U = I (matriz identidade)\n",
        "UtU = U.T @ U\n",
        "print(\"1Ô∏è‚É£ U^T * U (deve ser pr√≥ximo da identidade):\")\n",
        "print(np.round(UtU, 3))\n",
        "\n",
        "# 2. Verificar se V^T * V = I\n",
        "VtV = Vt @ Vt.T\n",
        "print(\"\\n2Ô∏è‚É£ V^T * V (deve ser pr√≥ximo da identidade):\")\n",
        "print(np.round(VtV, 3))\n",
        "\n",
        "# 3. Reconstruir a matriz original\n",
        "# Primeiro, vamos criar a matriz Sigma completa\n",
        "Sigma = np.zeros_like(A, dtype=float)\n",
        "np.fill_diagonal(Sigma, sigma)\n",
        "\n",
        "A_reconstruida = U @ Sigma @ Vt\n",
        "print(\"\\n3Ô∏è‚É£ Matriz original vs reconstru√≠da:\")\n",
        "print(\"Original:\")\n",
        "print(A)\n",
        "print(\"\\nReconstru√≠da:\")\n",
        "print(np.round(A_reconstruida, 3))\n",
        "\n",
        "# 4. Erro de reconstru√ß√£o\n",
        "erro = np.linalg.norm(A - A_reconstruida)\n",
        "print(f\"\\n4Ô∏è‚É£ Erro de reconstru√ß√£o: {erro:.10f}\")\n",
        "print(\"Liiindo! Praticamente zero! üéâ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. üìä Visualizando a SVD em A√ß√£o\n\nAgora vamos ver como a SVD \"enxerga\" os dados! Vou criar um exemplo visual para mostrar como ela identifica as dire√ß√µes principais nos dados.\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/algebra-linear-para-ia-modulo-09_img_01.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar dados 2D com padr√£o claro para visualizar a SVD\n",
        "np.random.seed(42)\n",
        "\n",
        "# Dados que t√™m uma dire√ß√£o principal clara\n",
        "n_points = 100\n",
        "x = np.random.randn(n_points)\n",
        "y = 2 * x + 0.5 * np.random.randn(n_points)  # y correlacionado com x\n",
        "\n",
        "# Matriz de dados (cada linha √© um ponto)\n",
        "dados = np.column_stack([x, y])\n",
        "\n",
        "# Aplicar SVD\n",
        "U_dados, sigma_dados, Vt_dados = np.linalg.svd(dados, full_matrices=False)\n",
        "\n",
        "# Visualiza√ß√£o\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gr√°fico 1: Dados originais com dire√ß√µes principais\n",
        "ax1.scatter(x, y, alpha=0.6, s=50)\n",
        "ax1.set_title('Dados Originais com Dire√ß√µes Principais da SVD')\n",
        "ax1.set_xlabel('X')\n",
        "ax1.set_ylabel('Y')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plotar as dire√ß√µes principais (vetores singulares)\n",
        "centro = np.mean(dados, axis=0)\n",
        "escala = 3\n",
        "\n",
        "# Primeira dire√ß√£o principal (maior valor singular)\n",
        "v1 = Vt_dados[0] * sigma_dados[0] * escala\n",
        "ax1.arrow(centro[0], centro[1], v1[0], v1[1], \n",
        "          head_width=0.3, head_length=0.2, fc='red', ec='red', linewidth=3,\n",
        "          label=f'1¬™ dire√ß√£o (œÉ={sigma_dados[0]:.2f})')\n",
        "\n",
        "# Segunda dire√ß√£o principal (menor valor singular)\n",
        "v2 = Vt_dados[1] * sigma_dados[1] * escala\n",
        "ax1.arrow(centro[0], centro[1], v2[0], v2[1], \n",
        "          head_width=0.3, head_length=0.2, fc='blue', ec='blue', linewidth=3,\n",
        "          label=f'2¬™ dire√ß√£o (œÉ={sigma_dados[1]:.2f})')\n",
        "\n",
        "ax1.legend()\n",
        "\n",
        "# Gr√°fico 2: Valores singulares\n",
        "ax2.bar(['œÉ‚ÇÅ', 'œÉ‚ÇÇ'], sigma_dados, color=['red', 'blue'], alpha=0.7)\n",
        "ax2.set_title('Valores Singulares (Import√¢ncia das Dire√ß√µes)')\n",
        "ax2.set_ylabel('Valor Singular')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"üéØ A primeira dire√ß√£o captura {(sigma_dados[0]**2 / np.sum(sigma_dados**2) * 100):.1f}% da vari√¢ncia!\")\n",
        "print(f\"üéØ A segunda dire√ß√£o captura {(sigma_dados[1]**2 / np.sum(sigma_dados**2) * 100):.1f}% da vari√¢ncia!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. üé® Compress√£o de Imagens com SVD\n\nAgora vem a parte mais legal! Vamos usar SVD para comprimir imagens. √â como se a gente tivesse um \"Instagram filter\" matem√°tico que remove o que n√£o √© essencial!\n\n### ü§î Como Funciona?\n\n1. **Decompomos** a imagem usando SVD: $A = U \\Sigma V^T$\n2. **Mantemos apenas** os $k$ maiores valores singulares\n3. **Reconstru√≠mos** usando: $A_k = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T$\n\n### üìê Taxa de Compress√£o\n\nPara uma imagem $m \\times n$, mantendo $k$ componentes:\n- **Original**: $m \\times n$ pixels\n- **Comprimida**: $k(m + n + 1)$ valores\n- **Taxa de compress√£o**: $\\frac{mn}{k(m+n+1)}$\n\n**üéØ Dica do Pedro:** A SVD √© perfeita para imagens porque muitas vezes os primeiros valores singulares capturam a ess√™ncia da imagem, e os demais s√£o s√≥ \"ru√≠do\"!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar uma imagem sint√©tica para demonstrar a compress√£o\n",
        "# (Em projetos reais, voc√™ carregaria uma imagem real)\n",
        "\n",
        "# Criando uma imagem sint√©tica com padr√µes interessantes\n",
        "x = np.linspace(0, 4*np.pi, 200)\n",
        "y = np.linspace(0, 4*np.pi, 200)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "\n",
        "# Imagem com padr√µes senoidais (simula texturas naturais)\n",
        "imagem = np.sin(X) * np.cos(Y) + 0.5 * np.sin(2*X + Y) + 0.3 * np.sin(X - 2*Y)\n",
        "# Normalizar para 0-255 (escala de cinza)\n",
        "imagem = ((imagem - imagem.min()) / (imagem.max() - imagem.min()) * 255).astype(np.uint8)\n",
        "\n",
        "print(f\"üì∏ Imagem criada com dimens√µes: {imagem.shape}\")\n",
        "print(f\"üíæ Tamanho original: {imagem.size} pixels\")\n",
        "\n",
        "# Aplicar SVD na imagem\n",
        "U_img, sigma_img, Vt_img = np.linalg.svd(imagem, full_matrices=False)\n",
        "\n",
        "print(f\"üîç N√∫mero de valores singulares: {len(sigma_img)}\")\n",
        "print(f\"üèÜ Maior valor singular: {sigma_img[0]:.2f}\")\n",
        "print(f\"ü•â Menor valor singular: {sigma_img[-1]:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fun√ß√£o para reconstruir imagem com k componentes\n",
        "def reconstruir_imagem_svd(U, sigma, Vt, k):\n",
        "    \"\"\"Reconstr√≥i imagem usando apenas os k primeiros componentes SVD\"\"\"\n",
        "    return (U[:, :k] @ np.diag(sigma[:k]) @ Vt[:k, :]).astype(np.uint8)\n",
        "\n",
        "# Testando diferentes n√≠veis de compress√£o\n",
        "componentes = [1, 5, 10, 20, 50, 100]\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, k in enumerate(componentes):\n",
        "    # Reconstruir imagem\n",
        "    img_comprimida = reconstruir_imagem_svd(U_img, sigma_img, Vt_img, k)\n",
        "    \n",
        "    # Calcular taxa de compress√£o\n",
        "    m, n = imagem.shape\n",
        "    tamanho_original = m * n\n",
        "    tamanho_comprimido = k * (m + n + 1)\n",
        "    taxa_compressao = tamanho_original / tamanho_comprimido\n",
        "    \n",
        "    # Calcular vari√¢ncia preservada\n",
        "    variancia_preservada = np.sum(sigma_img[:k]**2) / np.sum(sigma_img**2) * 100\n",
        "    \n",
        "    # Plotar\n",
        "    axes[i].imshow(img_comprimida, cmap='gray')\n",
        "    axes[i].set_title(f'k={k} componentes\\nCompress√£o: {taxa_compressao:.1f}x\\nVari√¢ncia: {variancia_preservada:.1f}%')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('üé® Compress√£o de Imagem usando SVD', fontsize=16, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéØ Observe como:\")\n",
        "print(\"- Com poucos componentes, vemos os padr√µes principais\")\n",
        "print(\"- Conforme aumentamos k, mais detalhes aparecem\")\n",
        "print(\"- A compress√£o vs qualidade √© um trade-off cl√°ssico!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos plotar como os valores singulares decaem\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gr√°fico 1: Valores singulares\n",
        "ax1.plot(sigma_img, linewidth=2, color='blue')\n",
        "ax1.set_title('Decaimento dos Valores Singulares')\n",
        "ax1.set_xlabel('√çndice do Componente')\n",
        "ax1.set_ylabel('Valor Singular')\n",
        "ax1.set_yscale('log')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Destacar alguns componentes importantes\n",
        "componentes_destacados = [0, 4, 9, 19, 49]\n",
        "ax1.scatter(componentes_destacados, sigma_img[componentes_destacados], \n",
        "           color='red', s=100, zorder=5)\n",
        "\n",
        "# Gr√°fico 2: Vari√¢ncia acumulada\n",
        "variancia_acumulada = np.cumsum(sigma_img**2) / np.sum(sigma_img**2) * 100\n",
        "ax2.plot(variancia_acumulada, linewidth=2, color='green')\n",
        "ax2.set_title('Vari√¢ncia Explicada Acumulada')\n",
        "ax2.set_xlabel('N√∫mero de Componentes')\n",
        "ax2.set_ylabel('Vari√¢ncia Explicada (%)')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Linha de 90% de vari√¢ncia\n",
        "idx_90 = np.where(variancia_acumulada >= 90)[0][0]\n",
        "ax2.axhline(y=90, color='red', linestyle='--', alpha=0.7)\n",
        "ax2.axvline(x=idx_90, color='red', linestyle='--', alpha=0.7)\n",
        "ax2.text(idx_90+5, 85, f'90% com {idx_90+1} componentes', \n",
        "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"üí° Para capturar 90% da vari√¢ncia, precisamos de apenas {idx_90+1} componentes!\")\n",
        "print(f\"üéØ Isso representa uma compress√£o de {len(sigma_img)/(idx_90+1):.1f}x!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. üé¨ Sistema de Recomenda√ß√£o com SVD\n\nAgora vamos para o Netflix da matem√°tica! üçø A SVD √© a base de muitos sistemas de recomenda√ß√£o, incluindo o famoso Netflix Prize!\n\n### üéØ Como Funciona?\n\n1. **Matriz Usu√°rio-Item**: Linhas = usu√°rios, Colunas = filmes, Valores = avalia√ß√µes\n2. **Problema**: Matriz tem muitos valores faltando (nem todo mundo viu todos os filmes)\n3. **Solu√ß√£o**: SVD encontra padr√µes latentes (g√™neros, prefer√™ncias)\n4. **Predi√ß√£o**: Usa os padr√µes para prever avalia√ß√µes faltantes\n\n### üßÆ A Matem√°tica\n\nSe $R$ √© nossa matriz de avalia√ß√µes:\n\n$$R \\approx U_k \\Sigma_k V_k^T$$\n\nOnde:\n- $U_k$: Prefer√™ncias dos usu√°rios por fatores latentes\n- $\\Sigma_k$: Import√¢ncia de cada fator\n- $V_k^T$: Como cada filme se relaciona com os fatores\n\n**üéØ Dica do Pedro:** Os fatores latentes s√£o como \"g√™neros abstratos\" - podem ser a√ß√£o, romance, sci-fi, ou combina√ß√µes mais complexas que s√≥ a matem√°tica consegue encontrar!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um dataset sint√©tico de avalia√ß√µes de filmes\n",
        "np.random.seed(42)\n",
        "\n",
        "# Par√¢metros do sistema\n",
        "n_usuarios = 50\n",
        "n_filmes = 30\n",
        "n_fatores_reais = 3  # Vamos simular 3 \"g√™neros\" latentes\n",
        "\n",
        "# Criar fatores latentes verdadeiros\n",
        "# Usu√°rios: prefer√™ncias por cada fator (0-1)\n",
        "usuarios_fatores = np.random.beta(2, 2, (n_usuarios, n_fatores_reais))\n",
        "\n",
        "# Filmes: intensidade de cada fator (0-1)\n",
        "filmes_fatores = np.random.beta(2, 2, (n_filmes, n_fatores_reais))\n",
        "\n",
        "# Gerar avalia√ß√µes baseadas nos fatores latentes\n",
        "avaliacoes_completas = usuarios_fatores @ filmes_fatores.T\n",
        "\n",
        "# Normalizar para escala 1-5 (como avalia√ß√µes reais)\n",
        "avaliacoes_completas = 1 + 4 * avaliacoes_completas\n",
        "\n",
        "# Simular dados faltantes (nem todo mundo viu todos os filmes)\n",
        "mascara_observada = np.random.random((n_usuarios, n_filmes)) < 0.3  # 30% de dados observados\n",
        "\n",
        "# Matriz com dados faltantes (NaN onde n√£o foi avaliado)\n",
        "avaliacoes_observadas = avaliacoes_completas.copy()\n",
        "avaliacoes_observadas[~mascara_observada] = np.nan\n",
        "\n",
        "print(f\"üé¨ Sistema de Recomenda√ß√£o Criado!\")\n",
        "print(f\"üë• Usu√°rios: {n_usuarios}\")\n",
        "print(f\"üéûÔ∏è Filmes: {n_filmes}\")\n",
        "print(f\"üìä Avalia√ß√µes observadas: {np.sum(mascara_observada)} de {n_usuarios * n_filmes} ({np.mean(mascara_observada)*100:.1f}%)\")\n",
        "print(f\"üé≠ Fatores latentes reais: {n_fatores_reais}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Para aplicar SVD, precisamos lidar com valores faltantes\n",
        "# Estrat√©gia simples: substituir NaN pela m√©dia de cada filme\n",
        "\n",
        "def preencher_com_media(matriz):\n",
        "    \"\"\"Preenche valores NaN com a m√©dia de cada coluna (filme)\"\"\"\n",
        "    matriz_preenchida = matriz.copy()\n",
        "    for j in range(matriz.shape[1]):\n",
        "        coluna = matriz[:, j]\n",
        "        media_coluna = np.nanmean(coluna)\n",
        "        matriz_preenchida[np.isnan(coluna), j] = media_coluna\n",
        "    return matriz_preenchida\n",
        "\n",
        "# Preencher dados faltantes\n",
        "avaliacoes_preenchidas = preencher_com_media(avaliacoes_observadas)\n",
        "\n",
        "# Aplicar SVD\n",
        "U_rec, sigma_rec, Vt_rec = np.linalg.svd(avaliacoes_preenchidas, full_matrices=False)\n",
        "\n",
        "print(\"üìà SVD aplicada no sistema de recomenda√ß√£o!\")\n",
        "print(f\"üîç Valores singulares encontrados: {len(sigma_rec)}\")\n",
        "\n",
        "# Visualizar os valores singulares\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(sigma_rec[:15], 'bo-', linewidth=2, markersize=8)\n",
        "plt.title('Primeiros 15 Valores Singulares')\n",
        "plt.xlabel('Componente')\n",
        "plt.ylabel('Valor Singular')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Destacar os 3 primeiros (esperamos que capturem os fatores reais)\n",
        "plt.scatter([0, 1, 2], sigma_rec[:3], color='red', s=100, zorder=5)\n",
        "plt.text(0.5, sigma_rec[0]*0.9, '3 fatores\\nlatentes?', \n",
        "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "variancia_acum_rec = np.cumsum(sigma_rec**2) / np.sum(sigma_rec**2) * 100\n",
        "plt.plot(variancia_acum_rec[:15], 'go-', linewidth=2, markersize=8)\n",
        "plt.title('Vari√¢ncia Explicada Acumulada')\n",
        "plt.xlabel('N√∫mero de Componentes')\n",
        "plt.ylabel('Vari√¢ncia Explicada (%)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüéØ Os primeiros 3 componentes explicam {variancia_acum_rec[2]:.1f}% da vari√¢ncia!\")\n",
        "print(f\"üéâ Como esperado, j√° que simulamos 3 fatores latentes!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fazer predi√ß√µes usando SVD com k=3 componentes\n",
        "k = 3\n",
        "avaliacoes_preditas = U_rec[:, :k] @ np.diag(sigma_rec[:k]) @ Vt_rec[:k, :]\n",
        "\n",
        "# Avaliar qualidade das predi√ß√µes\n",
        "# Vamos calcular erro apenas nos dados que foram \"escondidos\"\n",
        "mascara_teste = ~mascara_observada  # Dados que o modelo n√£o viu\n",
        "\n",
        "# RMSE (Root Mean Square Error)\n",
        "erro_quadratico = (avaliacoes_completas[mascara_teste] - avaliacoes_preditas[mascara_teste])**2\n",
        "rmse = np.sqrt(np.mean(erro_quadratico))\n",
        "\n",
        "print(f\"üéØ Avalia√ß√£o do Sistema de Recomenda√ß√£o:\")\n",
        "print(f\"üìä RMSE: {rmse:.3f} (escala 1-5)\")\n",
        "print(f\"üìà Erro m√©dio: {np.mean(np.abs(avaliacoes_completas[mascara_teste] - avaliacoes_preditas[mascara_teste])):.3f}\")\n",
        "\n",
        "# Vamos ver alguns exemplos de predi√ß√µes\n",
        "print(\"\\nüîç Exemplos de Predi√ß√µes vs Realidade:\")\n",
        "print(\"(Para filmes que os usu√°rios n√£o avaliaram originalmente)\\n\")\n",
        "\n",
        "exemplos = np.where(mascara_teste)\n",
        "for i in range(min(10, len(exemplos[0]))):\n",
        "    usuario = exemplos[0][i]\n",
        "    filme = exemplos[1][i]\n",
        "    real = avaliacoes_completas[usuario, filme]\n",
        "    predito = avaliacoes_preditas[usuario, filme]\n",
        "    print(f\"üë§ Usu√°rio {usuario:2d}, üé¨ Filme {filme:2d}: Real={real:.2f}, Predito={predito:.2f}, Erro={abs(real-predito):.2f}\")\n",
        "\n",
        "print(f\"\\nüéâ Liiindo! A SVD conseguiu capturar os padr√µes de prefer√™ncia!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. üìä Redu√ß√£o de Dimensionalidade: PCA vs SVD\n\nT√°, mas qual a diferen√ßa entre SVD e PCA? Spoiler: eles s√£o primos! üë®‚Äçüë©‚Äçüëß‚Äçüë¶\n\n### üîó A Conex√£o\n\nO PCA (que vamos ver no pr√≥ximo m√≥dulo) √© basicamente SVD aplicada na matriz de covari√¢ncia:\n\n1. **PCA**: Encontra dire√ß√µes de m√°xima vari√¢ncia\n2. **SVD**: Decomp√µe qualquer matriz em dire√ß√µes principais\n3. **Conex√£o**: PCA pode ser calculado via SVD!\n\n### üéØ Quando Usar Cada Um?\n\n- **SVD**: Dados faltantes, matrizes n√£o-quadradas, sistemas de recomenda√ß√£o\n- **PCA**: Redu√ß√£o de dimensionalidade cl√°ssica, an√°lise de componentes principais\n\n**üéØ Dica do Pedro:** SVD √© mais geral e poderosa, PCA √© mais espec√≠fica para an√°lise de vari√¢ncia. √â como comparar um canivete su√≠√ßo com uma chave de fenda especializada!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar dados de alta dimensionalidade para demonstrar redu√ß√£o\n",
        "# Simulando dados de sensores, pixels, features, etc.\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Criar dados com estrutura latente\n",
        "n_amostras = 200\n",
        "n_dimensoes = 50\n",
        "n_fatores_latentes = 5\n",
        "\n",
        "# Fatores latentes (vari√°veis n√£o observadas que geram os dados)\n",
        "fatores_latentes = np.random.randn(n_amostras, n_fatores_latentes)\n",
        "\n",
        "# Matriz de carregamentos (como cada fator afeta cada dimens√£o)\n",
        "carregamentos = np.random.randn(n_fatores_latentes, n_dimensoes)\n",
        "\n",
        "# Gerar dados de alta dimensionalidade\n",
        "dados_alta_dim = fatores_latentes @ carregamentos + 0.1 * np.random.randn(n_amostras, n_dimensoes)\n",
        "\n",
        "print(f\"üìä Dados de alta dimensionalidade criados:\")\n",
        "print(f\"üìè Dimens√µes originais: {dados_alta_dim.shape}\")\n",
        "print(f\"üéØ Fatores latentes reais: {n_fatores_latentes}\")\n",
        "\n",
        "# Centralizar os dados (importante para PCA/SVD)\n",
        "dados_centralizados = dados_alta_dim - np.mean(dados_alta_dim, axis=0)\n",
        "\n",
        "# Aplicar SVD\n",
        "U_dim, sigma_dim, Vt_dim = np.linalg.svd(dados_centralizados, full_matrices=False)\n",
        "\n",
        "# Calcular vari√¢ncia explicada por cada componente\n",
        "variancia_explicada = (sigma_dim**2) / np.sum(sigma_dim**2) * 100\n",
        "variancia_acumulada = np.cumsum(variancia_explicada)\n",
        "\n",
        "# Visualizar\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Vari√¢ncia explicada por componente\n",
        "ax1.bar(range(1, 16), variancia_explicada[:15], alpha=0.7, color='skyblue')\n",
        "ax1.bar(range(1, 6), variancia_explicada[:5], alpha=0.9, color='red', \n",
        "        label='5 primeiros (fatores reais)')\n",
        "ax1.set_title('Vari√¢ncia Explicada por Componente')\n",
        "ax1.set_xlabel('Componente')\n",
        "ax1.set_ylabel('Vari√¢ncia Explicada (%)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Vari√¢ncia acumulada\n",
        "ax2.plot(range(1, 21), variancia_acumulada[:20], 'bo-', linewidth=2)\n",
        "ax2.axhline(y=95, color='red', linestyle='--', alpha=0.7, label='95% da vari√¢ncia')\n",
        "ax2.axvline(x=5, color='green', linestyle='--', alpha=0.7, label='5 componentes (real)')\n",
        "ax2.set_title('Vari√¢ncia Explicada Acumulada')\n",
        "ax2.set_xlabel('N√∫mero de Componentes')\n",
        "ax2.set_ylabel('Vari√¢ncia Acumulada (%)')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüéØ Os primeiros 5 componentes explicam {variancia_acumulada[4]:.1f}% da vari√¢ncia!\")\n",
        "print(f\"üéâ Reduzimos de {n_dimensoes} para 5 dimens√µes mantendo quase toda informa√ß√£o!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. üî• Exerc√≠cio Pr√°tico: Netflix Simplificado\n\nAgora √© sua vez! Vamos criar um mini-Netflix usando SVD! üçø\n\n**üéØ Seu Desafio:**\n1. Complete o c√≥digo para criar um sistema de recomenda√ß√£o\n2. Teste diferentes n√∫meros de componentes\n3. Avalie a qualidade das predi√ß√µes\n4. Fa√ßa recomenda√ß√µes para um usu√°rio espec√≠fico\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/algebra-linear-para-ia-modulo-09_img_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üé¨ EXERC√çCIO: Construa seu pr√≥prio Netflix!\n",
        "\n",
        "# Dados do mini-Netflix\n",
        "filmes = ['A√ß√£o 1', 'A√ß√£o 2', 'Romance 1', 'Romance 2', 'Com√©dia 1', 'Com√©dia 2', 'Terror 1', 'Terror 2']\n",
        "usuarios = ['Alice', 'Bob', 'Carol', 'David', 'Eva']\n",
        "\n",
        "# Matriz de avalia√ß√µes (5 usu√°rios x 8 filmes, escala 1-5, NaN = n√£o assistiu)\n",
        "avaliacoes = np.array([\n",
        "    [5, 4, 1, 2, 3, 3, 1, 1],  # Alice: gosta de a√ß√£o, n√£o gosta de terror\n",
        "    [4, 5, 2, 1, 4, 4, 2, 1],  # Bob: a√ß√£o e com√©dia\n",
        "    [1, 2, 5, 4, 2, 3, 1, 2],  # Carol: romance\n",
        "    [2, 1, 4, 5, 1, 2, 4, 5],  # David: romance e terror\n",
        "    [3, 4, 3, 2, 5, 4, 2, 1],  # Eva: com√©dia\n",
        "], dtype=float)\n",
        "\n",
        "# Simular dados faltantes (alguns filmes n√£o foram assistidos)\n",
        "np.random.seed(42)\n",
        "mascara_faltantes = np.random.random(avaliacoes.shape) < 0.3  # 30% de dados faltantes\n",
        "avaliacoes_com_faltantes = avaliacoes.copy()\n",
        "avaliacoes_com_faltantes[mascara_faltantes] = np.nan\n",
        "\n",
        "print(\"üé¨ Mini-Netflix Dataset:\")\n",
        "print(\"\\nAvalia√ß√µes observadas (NaN = n√£o assistiu):\")\n",
        "df_avaliacoes = pd.DataFrame(avaliacoes_com_faltantes, index=usuarios, columns=filmes)\n",
        "print(df_avaliacoes)\n",
        "\n",
        "# TODO: Complete as fun√ß√µes abaixo!\n",
        "\n",
        "def preencher_dados_faltantes(matriz):\n",
        "    \"\"\"\n",
        "    Preenche dados faltantes com a m√©dia de cada filme\n",
        "    TODO: Implemente esta fun√ß√£o!\n",
        "    \"\"\"\n",
        "    # Sua implementa√ß√£o aqui\n",
        "    matriz_preenchida = matriz.copy()\n",
        "    for j in range(matriz.shape[1]):\n",
        "        coluna = matriz[:, j]\n",
        "        media = np.nanmean(coluna)\n",
        "        matriz_preenchida[np.isnan(coluna), j] = media\n",
        "    return matriz_preenchida\n",
        "\n",
        "def aplicar_svd_recomendacao(matriz, k=2):\n",
        "    \"\"\"\n",
        "    Aplica SVD e reconstr√≥i matriz com k componentes\n",
        "    TODO: Complete esta fun√ß√£o!\n",
        "    \"\"\"\n",
        "    # Preencher dados faltantes\n",
        "    matriz_preenchida = preencher_dados_faltantes(matriz)\n",
        "    \n",
        "    # Aplicar SVD\n",
        "    U, sigma, Vt = np.linalg.svd(matriz_preenchida, full_matrices=False)\n",
        "    \n",
        "    # Reconstruir com k componentes\n",
        "    matriz_reconstruida = U[:, :k] @ np.diag(sigma[:k]) @ Vt[:k, :]\n",
        "    \n",
        "    return matriz_reconstruida, sigma\n",
        "\n",
        "# Testar sua implementa√ß√£o\n",
        "predicoes, valores_singulares = aplicar_svd_recomendacao(avaliacoes_com_faltantes, k=3)\n",
        "\n",
        "print(\"\\nüîÆ Predi√ß√µes do sistema:\")\n",
        "df_predicoes = pd.DataFrame(np.round(predicoes, 2), index=usuarios, columns=filmes)\n",
        "print(df_predicoes)\n",
        "\n",
        "print(f\"\\nüìä Valores singulares: {np.round(valores_singulares, 2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ PARTE 2 DO EXERC√çCIO: An√°lise e Recomenda√ß√µes\n",
        "\n",
        "def recomendar_filmes(usuario_idx, avaliacoes_originais, predicoes, filmes, top_k=3):\n",
        "    \"\"\"\n",
        "    Recommenda filmes que o usu√°rio n√£o assistiu\n",
        "    TODO: Complete esta fun√ß√£o!\n",
        "    \"\"\"\n",
        "    # Identificar filmes n√£o assistidos (eram NaN)\n",
        "    nao_assistidos = np.isnan(avaliacoes_originais[usuario_idx])\n",
        "    \n",
        "    # Pegar predi√ß√µes para filmes n√£o assistidos\n",
        "    predicoes_nao_assistidos = predicoes[usuario_idx][nao_assistidos]\n",
        "    filmes_nao_assistidos = np.array(filmes)[nao_assistidos]\n",
        "    \n",
        "    # Ordenar por predi√ß√£o (maior para menor)\n",
        "    indices_ordenados = np.argsort(predicoes_nao_assistidos)[::-1]\n",
        "    \n",
        "    # Retornar top k recomenda√ß√µes\n",
        "    top_filmes = filmes_nao_assistidos[indices_ordenados[:top_k]]\n",
        "    top_predicoes = predicoes_nao_assistidos[indices_ordenados[:top_k]]\n",
        "    \n",
        "    return list(zip(top_filmes, top_predicoes))\n",
        "\n",
        "# Testar recomenda√ß√µes para cada usu√°rio\n",
        "print(\"üé¨ RECOMENDA√á√ïES PERSONALIZADAS:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, usuario in enumerate(usuarios):\n",
        "    recomendacoes = recomendar_filmes(i, avaliacoes_com_faltantes, predicoes, filmes)\n",
        "    \n",
        "    print(f\"\\nüë§ {usuario}:\")\n",
        "    print(f\"   Filmes j√° assistidos: {[filmes[j] for j in range(len(filmes)) if not np.isnan(avaliacoes_com_faltantes[i, j])]}\")\n",
        "    print(f\"   üî• Recomenda√ß√µes:\")\n",
        "    for j, (filme, predicao) in enumerate(recomendacoes, 1):\n",
        "        print(f\"      {j}. {filme} (predi√ß√£o: {predicao:.2f})\")\n",
        "\n",
        "# Visualizar matriz de correla√ß√£o dos padr√µes encontrados\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.heatmap(avaliacoes, annot=True, cmap='RdYlBu_r', center=3, \n",
        "            xticklabels=filmes, yticklabels=usuarios, cbar_kws={'label': 'Avalia√ß√£o'})\n",
        "plt.title('Matriz Real (sem dados faltantes)')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "sns.heatmap(predicoes, annot=True, fmt='.1f', cmap='RdYlBu_r', center=3,\n",
        "            xticklabels=filmes, yticklabels=usuarios, cbar_kws={'label': 'Predi√ß√£o'})\n",
        "plt.title('Predi√ß√µes SVD')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.bar(range(len(valores_singulares)), valores_singulares, alpha=0.7)\n",
        "plt.title('Valores Singulares (Fatores Latentes)')\n",
        "plt.xlabel('Componente')\n",
        "plt.ylabel('Valor Singular')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "# Erro por usu√°rio\n",
        "erros_usuario = []\n",
        "for i in range(len(usuarios)):\n",
        "    mask_observados = ~np.isnan(avaliacoes_com_faltantes[i])\n",
        "    if np.any(mask_observados):\n",
        "        erro = np.mean(np.abs(avaliacoes[i][mask_observados] - predicoes[i][mask_observados]))\n",
        "        erros_usuario.append(erro)\n",
        "    else:\n",
        "        erros_usuario.append(0)\n",
        "\n",
        "plt.bar(usuarios, erros_usuario, alpha=0.7, color='orange')\n",
        "plt.title('Erro M√©dio por Usu√°rio')\n",
        "plt.ylabel('MAE (Mean Absolute Error)')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüéâ Parab√©ns! Voc√™ criou seu pr√≥prio sistema de recomenda√ß√£o!\")\n",
        "print(f\"üìä Erro m√©dio geral: {np.mean(erros_usuario):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. üöÄ SVD Truncada: Otimizando para Big Data\n\nQuando seus dados s√£o ENORMES (tipo Instagram com bilh√µes de fotos), a SVD cl√°ssica pode ser lenta demais. Entra em cena a **SVD Truncada**! ‚ö°\n\n### ü§î O Que √â?\n\nEm vez de calcular TODOS os valores singulares, calculamos apenas os $k$ maiores:\n- **SVD Completa**: $O(min(m^2n, mn^2))$ - muito lenta!\n- **SVD Truncada**: $O(k \\cdot min(m,n))$ - muito mais r√°pida!\n\n### üîß Implementa√ß√µes Populares\n- **scikit-learn**: `TruncatedSVD`\n- **scipy**: `svds` (sparse SVD)\n- **randomized SVD**: Para matrizes muito grandes\n\n**üéØ Dica do Pedro:** Para dados reais, sempre use SVD truncada! √â como pedir apenas as fatias mais gostosas da pizza em vez de comer a pizza inteira! üçï"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparando SVD completa vs truncada\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse.linalg import svds\n",
        "import time\n",
        "\n",
        "# Criar matriz grande para testar performance\n",
        "np.random.seed(42)\n",
        "m, n = 1000, 500\n",
        "matriz_grande = np.random.randn(m, n)\n",
        "\n",
        "print(f\"üî• Testando performance em matriz {m}x{n}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. SVD completa (NumPy)\n",
        "start_time = time.time()\n",
        "U_completa, sigma_completa, Vt_completa = np.linalg.svd(matriz_grande, full_matrices=False)\n",
        "tempo_completa = time.time() - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è SVD Completa (NumPy): {tempo_completa:.3f}s\")\n",
        "print(f\"üìä Componentes calculados: {len(sigma_completa)}\")\n",
        "\n",
        "# 2. SVD truncada (scikit-learn)\n",
        "k = 50  # Queremos apenas 50 componentes\n",
        "start_time = time.time()\n",
        "svd_truncada = TruncatedSVD(n_components=k, random_state=42)\n",
        "U_truncada = svd_truncada.fit_transform(matriz_grande)\n",
        "sigma_truncada = svd_truncada.singular_values_\n",
        "Vt_truncada = svd_truncada.components_\n",
        "tempo_truncada = time.time() - start_time\n",
        "\n",
        "print(f\"‚ö° SVD Truncada (sklearn): {tempo_truncada:.3f}s\")\n",
        "print(f\"üìä Componentes calculados: {k}\")\n",
        "print(f\"üöÄ Speedup: {tempo_completa/tempo_truncada:.1f}x mais r√°pida!\")\n",
        "\n",
        "# 3. Comparar precis√£o dos valores singulares\n",
        "erro_valores = np.abs(sigma_completa[:k] - sigma_truncada)\n",
        "print(f\"\\nüéØ Erro m√©dio nos valores singulares: {np.mean(erro_valores):.6f}\")\n",
        "print(f\"üìà Erro m√°ximo: {np.max(erro_valores):.6f}\")\n",
        "\n",
        "# Visualizar compara√ß√£o\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Comparar valores singulares\n",
        "ax1.plot(sigma_completa[:k], 'b-', linewidth=2, label='SVD Completa', alpha=0.7)\n",
        "ax1.plot(sigma_truncada, 'r--', linewidth=2, label='SVD Truncada', alpha=0.7)\n",
        "ax1.set_title('Compara√ß√£o dos Valores Singulares')\n",
        "ax1.set_xlabel('Componente')\n",
        "ax1.set_ylabel('Valor Singular')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Comparar tempos\n",
        "metodos = ['SVD\\nCompleta', 'SVD\\nTruncada']\n",
        "tempos = [tempo_completa, tempo_truncada]\n",
        "cores = ['blue', 'red']\n",
        "\n",
        "bars = ax2.bar(metodos, tempos, color=cores, alpha=0.7)\n",
        "ax2.set_title('Compara√ß√£o de Performance')\n",
        "ax2.set_ylabel('Tempo (segundos)')\n",
        "\n",
        "# Adicionar valores nas barras\n",
        "for bar, tempo in zip(bars, tempos):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{tempo:.3f}s', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüí° Vari√¢ncia explicada pelos {k} componentes: {svd_truncada.explained_variance_ratio_.sum()*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. üé≠ Interpretando os Fatores Latentes\n\nUma das partes mais legais da SVD √© tentar entender o que os fatores latentes representam! √â como ser um detetive matem√°tico! üïµÔ∏è‚Äç‚ôÇÔ∏è\n\n### üîç O Que Procurar?\n\n1. **Padr√µes nos vetores singulares**: Quais vari√°veis t√™m pesos altos juntas?\n2. **Magnitude dos valores singulares**: Qu√£o importante √© cada fator?\n3. **Interpreta√ß√£o do dom√≠nio**: O que faz sentido no contexto dos dados?\n\n### üé¨ Exemplo: Sistema de Filmes\n- **Fator 1**: Pode ser \"A√ß√£o vs Romance\"\n- **Fator 2**: Pode ser \"Mainstream vs Indie\"\n- **Fator 3**: Pode ser \"Cl√°ssico vs Moderno\"\n\n**üéØ Dica do Pedro:** Os fatores latentes s√£o como \"DNA dos dados\" - capturam a ess√™ncia que nossos olhos n√£o conseguem ver diretamente!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos criar um exemplo mais elaborado para interpretar fatores latentes\n",
        "# Simulando dados de avalia√ß√£o de produtos com caracter√≠sticas conhecidas\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Produtos com caracter√≠sticas conhecidas\n",
        "produtos = {\n",
        "    'iPhone': [9, 2, 8, 9, 7],      # [tecnologia, pre√ßo_baixo, design, marca, durabilidade]\n",
        "    'Samsung': [8, 3, 7, 7, 8],\n",
        "    'Xiaomi': [7, 8, 6, 5, 6],\n",
        "    'Motorola': [6, 7, 5, 6, 7],\n",
        "    'Notebook_Gaming': [9, 1, 6, 8, 8],\n",
        "    'Notebook_Basico': [4, 9, 4, 4, 5],\n",
        "    'Tablet_Pro': [8, 2, 9, 8, 7],\n",
        "    'Tablet_Kids': [3, 8, 5, 3, 6],\n",
        "    'Smartwatch': [7, 4, 8, 7, 6],\n",
        "    'Fones_Premium': [6, 2, 9, 9, 8]\n",
        "}\n",
        "\n",
        "caracteristicas = ['Tecnologia', 'Custo-Benef√≠cio', 'Design', 'Marca', 'Durabilidade']\n",
        "nomes_produtos = list(produtos.keys())\n",
        "\n",
        "# Converter para matriz\n",
        "matriz_produtos = np.array(list(produtos.values()))\n",
        "\n",
        "print(\"üõçÔ∏è Matriz de Caracter√≠sticas dos Produtos:\")\n",
        "df_produtos = pd.DataFrame(matriz_produtos, index=nomes_produtos, columns=caracteristicas)\n",
        "print(df_produtos)\n",
        "\n",
        "# Aplicar SVD\n",
        "U_prod, sigma_prod, Vt_prod = np.linalg.svd(matriz_produtos, full_matrices=False)\n",
        "\n",
        "print(f\"\\nüîç An√°lise SVD:\")\n",
        "print(f\"üìä Valores singulares: {np.round(sigma_prod, 2)}\")\n",
        "\n",
        "# Analisar os primeiros fatores latentes\n",
        "n_fatores = 3\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Import√¢ncia dos fatores\n",
        "variancia_prod = (sigma_prod**2) / np.sum(sigma_prod**2) * 100\n",
        "axes[0,0].bar(range(1, len(sigma_prod)+1), variancia_prod, alpha=0.7)\n",
        "axes[0,0].set_title('Import√¢ncia dos Fatores Latentes')\n",
        "axes[0,0].set_xlabel('Fator')\n",
        "axes[0,0].set_ylabel('Vari√¢ncia Explicada (%)')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Primeiro fator latente (caracter√≠sticas)\n",
        "fator1_caract = Vt_prod[0, :]\n",
        "cores = ['red' if x > 0 else 'blue' for x in fator1_caract]\n",
        "axes[0,1].bar(caracteristicas, fator1_caract, color=cores, alpha=0.7)\n",
        "axes[0,1].set_title('Fator 1: Carregamentos das Caracter√≠sticas')\n",
        "axes[0,1].set_ylabel('Carregamento')\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Primeiro fator latente (produtos)\n",
        "fator1_prod = U_prod[:, 0] * sigma_prod[0]\n",
        "cores_prod = ['red' if x > 0 else 'blue' for x in fator1_prod]\n",
        "axes[1,0].barh(nomes_produtos, fator1_prod, color=cores_prod, alpha=0.7)\n",
        "axes[1,0].set_title('Fator 1: Scores dos Produtos')\n",
        "axes[1,0].set_xlabel('Score do Fator')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Visualiza√ß√£o 2D dos primeiros dois fatores\n",
        "fator1_prod = U_prod[:, 0] * sigma_prod[0]\n",
        "fator2_prod = U_prod[:, 1] * sigma_prod[1]\n",
        "\n",
        "axes[1,1].scatter(fator1_prod, fator2_prod, s=100, alpha=0.7)\n",
        "for i, produto in enumerate(nomes_produtos):\n",
        "    axes[1,1].annotate(produto, (fator1_prod[i], fator2_prod[i]), \n",
        "                      xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "axes[1,1].set_title('Produtos no Espa√ßo dos 2 Primeiros Fatores')\n",
        "axes[1,1].set_xlabel(f'Fator 1 ({variancia_prod[0]:.1f}% var)')\n",
        "axes[1,1].set_ylabel(f'Fator 2 ({variancia_prod[1]:.1f}% var)')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Interpreta√ß√£o dos fatores\n",
        "print(\"\\nüé≠ INTERPRETA√á√ÉO DOS FATORES LATENTES:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i in range(min(3, len(sigma_prod))):\n",
        "    print(f\"\\nüîç FATOR {i+1} ({variancia_prod[i]:.1f}% da vari√¢ncia):\")\n",
        "    \n",
        "    # Caracter√≠sticas mais importantes\n",
        "    carregamentos = Vt_prod[i, :]\n",
        "    indices_ordenados = np.argsort(np.abs(carregamentos))[::-1]\n",
        "    \n",
        "    print(\"   Caracter√≠sticas mais relevantes:\")\n",
        "    for j in indices_ordenados[:3]:\n",
        "        sinal = '+' if carregamentos[j] > 0 else '-'\n",
        "        print(f\"   {sinal} {caracteristicas[j]}: {carregamentos[j]:.3f}\")\n",
        "    \n",
        "    # Produtos extremos\n",
        "    scores_produtos = U_prod[:, i] * sigma_prod[i]\n",
        "    produto_max = np.argmax(scores_produtos)\n",
        "    produto_min = np.argmin(scores_produtos)\n",
        "    \n",
        "    print(f\"   Produto mais positivo: {nomes_produtos[produto_max]} ({scores_produtos[produto_max]:.3f})\")\n",
        "    print(f\"   Produto mais negativo: {nomes_produtos[produto_min]} ({scores_produtos[produto_min]:.3f})\")\n",
        "\n",
        "print(\"\\nüéâ Liiindo! Conseguimos descobrir os fatores que diferenciam os produtos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. ‚ö° Exerc√≠cio Desafio: An√°lise de Sentimentos\n\nAgora o desafio final! Vamos usar SVD para an√°lise de sentimentos em reviews! üé≠\n\n**üéØ Seu Objetivo:**\n1. Criar uma matriz termo-documento\n2. Aplicar SVD para encontrar t√≥picos latentes\n3. Classificar sentimentos usando os fatores encontrados\n4. Interpretar os t√≥picos descobertos\n\n**üîß Dicas:**\n- Use bag-of-words ou TF-IDF\n- Experimente diferentes n√∫meros de componentes\n- Visualize os t√≥picos encontrados\n\n![](/Users/pedroguth/Downloads/Projetos/Book Maker/5-Imagens/algebra-linear-para-ia-modulo-09_img_03.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üé≠ DESAFIO: An√°lise de Sentimentos com SVD\n",
        "\n",
        "# Dataset sint√©tico de reviews de filmes\n",
        "reviews = {\n",
        "    \"Filme incr√≠vel, adorei a hist√≥ria e os personagens!\": 1,  # Positivo\n",
        "    \"Excelente cinematografia, muito bem dirigido.\": 1,\n",
        "    \"Hist√≥ria emocionante, chorei no final.\": 1,\n",
        "    \"√ìtimos efeitos especiais, recomendo!\": 1,\n",
        "    \"Perfeito para assistir com a fam√≠lia.\": 1,\n",
        "    \"Filme horr√≠vel, hist√≥ria sem sentido.\": 0,  # Negativo\n",
        "    \"Muito chato, quase dormi no cinema.\": 0,\n",
        "    \"P√©ssimos atores, roteiro terr√≠vel.\": 0,\n",
        "    \"Waste de tempo, n√£o recomendo.\": 0,\n",
        "    \"Boring e previs√≠vel demais.\": 0,\n",
        "    \"Filme mediano, alguns momentos bons.\": 1,  # Neutro/Positivo\n",
        "    \"Hist√≥ria interessante mas execu√ß√£o fraca.\": 0,  # Neutro/Negativo\n",
        "}\n",
        "\n",
        "textos = list(reviews.keys())\n",
        "sentimentos = list(reviews.values())\n",
        "\n",
        "print(f\"üìù Dataset de Reviews:\")\n",
        "print(f\"Total de reviews: {len(textos)}\")\n",
        "print(f\"Positivos: {sum(sentimentos)}, Negativos: {len(sentimentos) - sum(sentimentos)}\")\n",
        "\n",
        "# TODO: Implemente as fun√ß√µes abaixo!\n",
        "\n",
        "def criar_vocabulario(textos):\n",
        "    \"\"\"\n",
        "    Cria vocabul√°rio b√°sico removendo pontua√ß√£o e convertendo para min√∫sculas\n",
        "    TODO: Complete esta fun√ß√£o\n",
        "    \"\"\"\n",
        "    import re\n",
        "    vocabulario = set()\n",
        "    \n",
        "    for texto in textos:\n",
        "        # Remover pontua√ß√£o e converter para min√∫sculas\n",
        "        palavras = re.findall(r'\\b\\w+\\b', texto.lower())\n",
        "        vocabulario.update(palavras)\n",
        "    \n",
        "    return sorted(list(vocabulario))\n",
        "\n",
        "def criar_matriz_termo_documento(textos, vocabulario):\n",
        "    \"\"\"\n",
        "    Cria matriz termo-documento (bag of words)\n",
        "    TODO: Complete esta fun√ß√£o\n",
        "    \"\"\"\n",
        "    import re\n",
        "    matriz = np.zeros((len(textos), len(vocabulario)))\n",
        "    \n",
        "    for i, texto in enumerate(textos):\n",
        "        palavras = re.findall(r'\\b\\w+\\b', texto.lower())\n",
        "        for palavra in palavras:\n",
        "            if palavra in vocabulario:\n",
        "                j = vocabulario.index(palavra)\n",
        "                matriz[i, j] += 1\n",
        "    \n",
        "    return matriz\n",
        "\n",
        "# Criar representa√ß√£o vetorial dos textos\n",
        "vocab = criar_vocabulario(textos)\n",
        "matriz_docs = criar_matriz_termo_documento(textos, vocab)\n",
        "\n",
        "print(f\"\\nüî§ Vocabul√°rio criado:\")\n",
        "print(f\"Tamanho do vocabul√°rio: {len(vocab)}\")\n",
        "print(f\"Primeiras 10 palavras: {vocab[:10]}\")\n",
        "print(f\"\\nüìä Matriz termo-documento: {matriz_docs.shape}\")\n",
        "print(f\"Densidade: {np.count_nonzero(matriz_docs) / matriz_docs.size * 100:.1f}% (palavras √∫nicas)\")\n",
        "\n",
        "# Aplicar SVD\n",
        "U_text, sigma_text, Vt_text = np.linalg.svd(matriz_docs, full_matrices=False)\n",
        "\n",
        "print(f\"\\nüîç SVD aplicada:\")\n",
        "print(f\"Componentes: {len(sigma_text)}\")\n",
        "print(f\"Valores singulares: {np.round(sigma_text[:5], 2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise dos t√≥picos encontrados pela SVD\n",
        "\n",
        "def analisar_topicos_svd(Vt, vocabulario, n_topicos=3, n_palavras=5):\n",
        "    \"\"\"\n",
        "    Analisa os t√≥picos latentes encontrados pela SVD\n",
        "    TODO: Complete esta fun√ß√£o\n",
        "    \"\"\"\n",
        "    print(\"üé≠ T√ìPICOS LATENTES DESCOBERTOS PELA SVD:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    for i in range(min(n_topicos, Vt.shape[0])):\n",
        "        print(f\"\\nüìã T√≥pico {i+1}:\")\n",
        "        \n",
        "        # Palavras com maior peso positivo\n",
        "        carregamentos = Vt[i, :]\n",
        "        indices_pos = np.argsort(carregamentos)[::-1][:n_palavras]\n",
        "        indices_neg = np.argsort(carregamentos)[:n_palavras]\n",
        "        \n",
        "        print(\"   Palavras mais positivas:\")\n",
        "        for idx in indices_pos:\n",
        "            if carregamentos[idx] > 0:\n",
        "                print(f\"   + {vocabulario[idx]}: {carregamentos[idx]:.3f}\")\n",
        "        \n",
        "        print(\"   Palavras mais negativas:\")\n",
        "        for idx in indices_neg:\n",
        "            if carregamentos[idx] < 0:\n",
        "                print(f\"   - {vocabulario[idx]}: {carregamentos[idx]:.3f}\")\n",
        "\n",
        "# Analisar t√≥picos\n",
        "analisar_topicos_svd(Vt_text, vocab, n_topicos=3)\n",
        "\n",
        "# Projetar documentos no espa√ßo latente\n",
        "n_componentes = 2\n",
        "documentos_projetados = U_text[:, :n_componentes] @ np.diag(sigma_text[:n_componentes])\n",
        "\n",
        "# Visualizar documentos no espa√ßo latente\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Scatter plot dos documentos\n",
        "cores = ['red' if s == 0 else 'blue' for s in sentimentos]\n",
        "labels = ['Negativo' if s == 0 else 'Positivo' for s in sentimentos]\n",
        "\n",
        "scatter = ax1.scatter(documentos_projetados[:, 0], documentos_projetados[:, 1], \n",
        "                     c=cores, s=100, alpha=0.7)\n",
        "\n",
        "# Adicionar labels aos pontos\n",
        "for i, (x, y) in enumerate(documentos_projetados):\n",
        "    ax1.annotate(f'Doc{i+1}', (x, y), xytext=(5, 5), \n",
        "                textcoords='offset points', fontsize=8)\n",
        "\n",
        "ax1.set_title('Documentos no Espa√ßo Latente SVD')\n",
        "ax1.set_xlabel('Componente 1 (T√≥pico Principal)')\n",
        "ax1.set_ylabel('Componente 2 (Segundo T√≥pico)')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Criar legenda manual\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [Patch(facecolor='red', label='Negativo'),\n",
        "                  Patch(facecolor='blue', label='Positivo')]\n",
        "ax1.legend(handles=legend_elements)\n",
        "\n",
        "# Import√¢ncia dos componentes\n",
        "variancia_text = (sigma_text**2) / np.sum(sigma_text**2) * 100\n",
        "ax2.bar(range(1, min(8, len(sigma_text)+1)), variancia_text[:min(7, len(variancia_text))], \n",
        "        alpha=0.7, color='green')\n",
        "ax2.set_title('Import√¢ncia dos T√≥picos Latentes')\n",
        "ax2.set_xlabel('T√≥pico')\n",
        "ax2.set_ylabel('Vari√¢ncia Explicada (%)')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Avaliar separa√ß√£o dos sentimentos\n",
        "print(f\"\\nüéØ AN√ÅLISE DE SEPARA√á√ÉO:\")\n",
        "media_pos = np.mean(documentos_projetados[np.array(sentimentos) == 1], axis=0)\n",
        "media_neg = np.mean(documentos_projetados[np.array(sentimentos) == 0], axis=0)\n",
        "\n",
        "print(f\"üìä Centro dos positivos: [{media_pos[0]:.3f}, {media_pos[1]:.3f}]\")\n",
        "print(f\"üìä Centro dos negativos: [{media_neg[0]:.3f}, {media_neg[1]:.3f}]\")\n",
        "print(f\"üìè Dist√¢ncia entre centros: {np.linalg.norm(media_pos - media_neg):.3f}\")\n",
        "\n",
        "if np.linalg.norm(media_pos - media_neg) > 1:\n",
        "    print(\"üéâ Excelente! SVD conseguiu separar bem os sentimentos!\")\n",
        "else:\n",
        "    print(\"ü§î SVD teve dificuldade em separar os sentimentos neste dataset simples.\")\n",
        "\n",
        "print(f\"\\nüí° O primeiro componente explica {variancia_text[0]:.1f}% da vari√¢ncia!\")\n",
        "print(f\"üí° Os dois primeiros explicam {variancia_text[0] + variancia_text[1]:.1f}% da vari√¢ncia!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. üîÑ SVD vs PCA vs NMF: O Confronto dos Gigantes\n\nAntes de encerrar, vamos comparar a SVD com outras t√©cnicas de decomposi√ß√£o! √â como um UFC da matem√°tica! ü•ä\n\n### ü•ä SVD vs PCA vs NMF\n\n| T√©cnica | Quando Usar | Vantagens | Desvantagens |\n|---------|-------------|-----------|-------------|\n| **SVD** | Qualquer matriz, sistemas de recomenda√ß√£o | Universal, precisa | Pode ser lenta |\n| **PCA** | Redu√ß√£o de dimensionalidade, an√°lise de vari√¢ncia | Interpret√°vel, eficiente | S√≥ matrizes quadradas de covari√¢ncia |\n| **NMF** | Dados n√£o-negativos, t√≥picos em texto | Componentes interpret√°veis | S√≥ dados ‚â• 0 |\n\n### üéØ Conex√µes Matem√°ticas\n\n- **PCA ‚âà SVD** da matriz de covari√¢ncia\n- **SVD** √© mais geral que PCA\n- **NMF** for√ßa positividade nos componentes\n\n**üéØ Dica do Pedro:** SVD √© como um canivete su√≠√ßo - resolve quase tudo. PCA √© especialista em vari√¢ncia. NMF √© perfeito quando voc√™ quer componentes que fazem sentido intuitivo (como ingredientes de uma receita)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compara√ß√£o pr√°tica: SVD vs PCA vs NMF\n",
        "from sklearn.decomposition import PCA, NMF\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Criar dados teste (imagem simples)\n",
        "np.random.seed(42)\n",
        "n_pixels = 100\n",
        "imagem_teste = np.random.exponential(2, (n_pixels, n_pixels))  # Dados n√£o-negativos\n",
        "\n",
        "# Adicionar alguns padr√µes estruturados\n",
        "x, y = np.meshgrid(np.linspace(0, 10, n_pixels), np.linspace(0, 10, n_pixels))\n",
        "padrao1 = 5 * np.exp(-((x-3)**2 + (y-3)**2) / 4)  # Gaussiana\n",
        "padrao2 = 3 * np.exp(-((x-7)**2 + (y-7)**2) / 6)  # Outra gaussiana\n",
        "imagem_teste += padrao1 + padrao2\n",
        "\n",
        "print(f\"üñºÔ∏è Imagem teste criada: {imagem_teste.shape}\")\n",
        "print(f\"üìä Min: {imagem_teste.min():.2f}, Max: {imagem_teste.max():.2f}\")\n",
        "\n",
        "# Aplicar as tr√™s t√©cnicas\n",
        "n_components = 10\n",
        "\n",
        "# 1. SVD\n",
        "start_time = time.time()\n",
        "U_comp, sigma_comp, Vt_comp = np.linalg.svd(imagem_teste, full_matrices=False)\n",
        "imagem_svd = U_comp[:, :n_components] @ np.diag(sigma_comp[:n_components]) @ Vt_comp[:n_components, :]\n",
        "tempo_svd = time.time() - start_time\n",
        "\n",
        "# 2. PCA (aplicado nas linhas da imagem)\n",
        "start_time = time.time()\n",
        "pca = PCA(n_components=n_components)\n",
        "scaler = StandardScaler()\n",
        "imagem_scaled = scaler.fit_transform(imagem_teste)\n",
        "componentes_pca = pca.fit_transform(imagem_scaled)\n",
        "imagem_pca = scaler.inverse_transform(pca.inverse_transform(componentes_pca))\n",
        "tempo_pca = time.time() - start_time\n",
        "\n",
        "# 3. NMF (Non-negative Matrix Factorization)\n",
        "start_time = time.time()\n",
        "nmf = NMF(n_components=n_components, random_state=42, max_iter=1000)\n",
        "componentes_nmf = nmf.fit_transform(imagem_teste)\n",
        "imagem_nmf = componentes_nmf @ nmf.components_\n",
        "tempo_nmf = time.time() - start_time\n",
        "\n",
        "# Calcular erros de reconstru√ß√£o\n",
        "erro_svd = np.linalg.norm(imagem_teste - imagem_svd)\n",
        "erro_pca = np.linalg.norm(imagem_teste - imagem_pca)\n",
        "erro_nmf = np.linalg.norm(imagem_teste - imagem_nmf)\n",
        "\n",
        "print(f\"\\n‚ö° COMPARA√á√ÉO DE PERFORMANCE:\")\n",
        "print(f\"{'M√©todo':<8} {'Tempo (ms)':<12} {'Erro Recons.':<15} {'RMSE':<10}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'SVD':<8} {tempo_svd*1000:<12.1f} {erro_svd:<15.3f} {erro_svd/np.sqrt(imagem_teste.size):<10.4f}\")\n",
        "print(f\"{'PCA':<8} {tempo_pca*1000:<12.1f} {erro_pca:<15.3f} {erro_pca/np.sqrt(imagem_teste.size):<10.4f}\")\n",
        "print(f\"{'NMF':<8} {tempo_nmf*1000:<12.1f} {erro_nmf:<15.3f} {erro_nmf/np.sqrt(imagem_teste.size):<10.4f}\")\n",
        "\n",
        "# Visualizar resultados\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
        "\n",
        "# Original\n",
        "im1 = axes[0,0].imshow(imagem_teste, cmap='viridis')\n",
        "axes[0,0].set_title('Original')\n",
        "axes[0,0].axis('off')\n",
        "plt.colorbar(im1, ax=axes[0,0], fraction=0.046, pad=0.04)\n",
        "\n",
        "# SVD\n",
        "im2 = axes[0,1].imshow(imagem_svd, cmap='viridis')\n",
        "axes[0,1].set_title(f'SVD (erro: {erro_svd:.1f})')\n",
        "axes[0,1].axis('off')\n",
        "plt.colorbar(im2, ax=axes[0,1], fraction=0.046, pad=0.04)\n",
        "\n",
        "# PCA\n",
        "im3 = axes[1,0].imshow(imagem_pca, cmap='viridis')\n",
        "axes[1,0].set_title(f'PCA (erro: {erro_pca:.1f})')\n",
        "axes[1,0].axis('off')\n",
        "plt.colorbar(im3, ax=axes[1,0], fraction=0.046, pad=0.04)\n",
        "\n",
        "# NMF\n",
        "im4 = axes[1,1].imshow(imagem_nmf, cmap='viridis')\n",
        "axes[1,1].set_title(f'NMF (erro: {erro_nmf:.1f})')\n",
        "axes[1,1].axis('off')\n",
        "plt.colorbar(im4, ax=axes[1,1], fraction=0.046, pad=0.04)\n",
        "\n",
        "plt.suptitle(f'Compara√ß√£o: SVD vs PCA vs NMF ({n_components} componentes)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Comparar vari√¢ncia explicada\n",
        "var_svd = np.sum(sigma_comp[:n_components]**2) / np.sum(sigma_comp**2) * 100\n",
        "var_pca = np.sum(pca.explained_variance_ratio_) * 100\n",
        "var_nmf = 100 - (erro_nmf**2 / np.linalg.norm(imagem_teste)**2) * 100\n",
        "\n",
        "print(f\"\\nüìä VARI√ÇNCIA EXPLICADA:\")\n",
        "print(f\"SVD: {var_svd:.1f}%\")\n",
        "print(f\"PCA: {var_pca:.1f}%\")\n",
        "print(f\"NMF: {var_nmf:.1f}% (aproximado)\")\n",
        "\n",
        "print(f\"\\nüéØ Vencedor em precis√£o: {'SVD' if erro_svd <= min(erro_pca, erro_nmf) else 'PCA' if erro_pca <= erro_nmf else 'NMF'}\")\n",
        "print(f\"üèÉ Vencedor em velocidade: {'SVD' if tempo_svd <= min(tempo_pca, tempo_nmf) else 'PCA' if tempo_pca <= tempo_nmf else 'NMF'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. üéì Resumo: O Que Aprendemos Sobre SVD\n\nParab√©ns! Voc√™ completou uma jornada √©pica pela SVD! üéâ Vamos recapitular os pontos principais:\n\n### üßÆ **Fundamentos Matem√°ticos**\n- SVD decomp√µe qualquer matriz: $A = U \\Sigma V^T$\n- $U$: dire√ß√µes no espa√ßo das linhas\n- $\\Sigma$: import√¢ncia de cada dire√ß√£o (valores singulares)\n- $V^T$: dire√ß√µes no espa√ßo das colunas\n\n### üé® **Aplica√ß√µes Pr√°ticas**\n- **Compress√£o de Imagens**: Mant√©m qualidade com menos dados\n- **Sistemas de Recomenda√ß√£o**: Encontra padr√µes de prefer√™ncia\n- **Redu√ß√£o de Dimensionalidade**: Remove ru√≠do, mant√©m informa√ß√£o\n- **An√°lise de Texto**: Descobre t√≥picos latentes\n\n### ‚ö° **Otimiza√ß√µes**\n- **SVD Truncada**: Para datasets grandes\n- **Randomized SVD**: Para matrizes enormes\n- **Sparse SVD**: Para dados esparsos\n\n### üéØ **Dicas Importantes**\n1. SVD sempre existe (diferente de eigendecomposition)\n2. Valores singulares em ordem decrescente\n3. Primeiros componentes capturam padr√µes principais\n4. Trade-off entre compress√£o e qualidade\n\n### üîÆ **Pr√≥ximos Passos**\nNo pr√≥ximo m√≥dulo, vamos mergulhar em **Autovetores e Autovalores** - os primos da SVD que s√£o essenciais para PCA!\n\n**üéØ Dica Final do Pedro:** A SVD √© como um \"raio-X matem√°tico\" - ela revela a estrutura interna dos seus dados que voc√™ n√£o consegue ver a olho nu. Use com sabedoria e sempre experimente diferentes n√∫meros de componentes!\n\n### üìö **Para Estudar Mais**\n- Randomized SVD para Big Data\n- SVD em sistemas de recomenda√ß√£o reais\n- Conex√µes entre SVD e outras decomposi√ß√µes\n- SVD para processamento de sinais\n\nLiiindo! Agora voc√™ √© um expert em SVD! üöÄ‚ú®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéä PARAB√âNS! C√≥digo de celebra√ß√£o!\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import FancyBboxPatch\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Criar visualiza√ß√£o de celebra√ß√£o\n",
        "ax.text(0.5, 0.7, 'üéâ PARAB√âNS! üéâ', fontsize=40, ha='center', va='center', \n",
        "        transform=ax.transAxes, weight='bold')\n",
        "\n",
        "ax.text(0.5, 0.5, 'Voc√™ dominou a SVD!', fontsize=24, ha='center', va='center',\n",
        "        transform=ax.transAxes, style='italic')\n",
        "\n",
        "# Estat√≠sticas do notebook\n",
        "conceitos_aprendidos = [\n",
        "    '‚úÖ Decomposi√ß√£o SVD matem√°tica',\n",
        "    '‚úÖ Compress√£o de imagens',\n",
        "    '‚úÖ Sistemas de recomenda√ß√£o', \n",
        "    '‚úÖ Redu√ß√£o de dimensionalidade',\n",
        "    '‚úÖ An√°lise de sentimentos',\n",
        "    '‚úÖ Interpreta√ß√£o de fatores latentes',\n",
        "    '‚úÖ SVD vs PCA vs NMF',\n",
        "    '‚úÖ Otimiza√ß√µes e truques'\n",
        "]\n",
        "\n",
        "y_start = 0.35\n",
        "for i, conceito in enumerate(conceitos_aprendidos):\n",
        "    ax.text(0.1, y_start - i*0.03, conceito, fontsize=12, \n",
        "            transform=ax.transAxes, va='center')\n",
        "\n",
        "# Caixa de destaque\n",
        "bbox = FancyBboxPatch((0.05, 0.05), 0.9, 0.4, \n",
        "                      boxstyle=\"round,pad=0.02\", \n",
        "                      facecolor='lightblue', \n",
        "                      edgecolor='navy', \n",
        "                      alpha=0.3,\n",
        "                      transform=ax.transAxes)\n",
        "ax.add_patch(bbox)\n",
        "\n",
        "ax.text(0.5, 0.25, 'üöÄ Pr√≥ximo Destino: Autovetores e Autovalores! üöÄ', \n",
        "        fontsize=16, ha='center', va='center', transform=ax.transAxes, \n",
        "        weight='bold', color='navy')\n",
        "\n",
        "ax.text(0.5, 0.15, 'Voc√™ est√° preparado para descobrir os eixos de rota√ß√£o da IA!', \n",
        "        fontsize=14, ha='center', va='center', transform=ax.transAxes, \n",
        "        style='italic', color='navy')\n",
        "\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéì M√ìDULO 9 CONCLU√çDO COM SUCESSO! üéì\")\n",
        "print(\"=\"*60)\n",
        "print(\"üìä Voc√™ implementou:\")\n",
        "print(\"   ‚Ä¢ Sistema de recomenda√ß√£o completo\")\n",
        "print(\"   ‚Ä¢ Compressor de imagens\")\n",
        "print(\"   ‚Ä¢ Analisador de sentimentos\")\n",
        "print(\"   ‚Ä¢ Comparador de t√©cnicas de decomposi√ß√£o\")\n",
        "print(\"\\nüèÜ Conquistas desbloqueadas:\")\n",
        "print(\"   ü•á Mestre da Decomposi√ß√£o SVD\")\n",
        "print(\"   ü•à Expert em Sistemas de Recomenda√ß√£o\")\n",
        "print(\"   ü•â Especialista em Compress√£o de Dados\")\n",
        "print(\"\\nüéØ Pr√≥xima aventura: Autovetores e Autovalores!\")\n",
        "print(\"üìö Continue sua jornada na √Ålgebra Linear para IA!\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"At√© o pr√≥ximo m√≥dulo! üëã\")\n",
        "print(\"- Pedro Guth\")"
      ]
    }
  ]
}